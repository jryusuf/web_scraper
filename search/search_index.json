{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome!","text":""},{"location":"#about-this-site","title":"About This Site","text":"<p>This site hosts the comprehensive design documentation for a proposed scalable data ingestion and processing system. It details the architectural planning, technology choices, and operational strategies considered during its design phase.</p> <p>This documentation aims to provide a clear and structured overview of the system's intended functionality, components, and underlying principles.</p>"},{"location":"#purpose-of-this-document","title":"Purpose of this Document","text":"<p>This site serves as the central reference for understanding the system's design, covering key aspects such as:</p> <ul> <li>High-Level System Architecture</li> <li>Core Technology Stack Selections</li> <li>Data Flow and Component Interactions</li> <li>Strategies for Handling Operational Challenges</li> <li>Approaches to Scalability and Long-Term Efficiency</li> </ul> <p>The focus is on presenting the design choices and their justifications clearly.</p>"},{"location":"#how-to-navigate-this-documentation","title":"How to Navigate This Documentation","text":"<p>This site is organized into logical sections detailing different facets of the proposed system.</p> <p>Please use the main navigation sidebar (usually on the left) or the top navigation tabs to explore:</p> <ul> <li>Introduction: Sets the context and outlines the core design principles guiding the architecture. (Start here for background).</li> <li>Architecture: Provides the visual system overview and summarizes the chosen technologies.</li> <li>Data Ingestion (Scraping): Details the strategy and implementation for acquiring data from external sources.</li> <li>Components &amp; Workflow: Describes the role and interaction of each individual system part.</li> <li>Data Processing &amp; Analysis: Outlines the plan for cleaning, standardizing, and utilizing the collected data.</li> <li>Operations &amp; Scalability: Covers deployment, monitoring, error handling, and efficiency strategies.</li> <li>Conclusion: Summarizes the design and discusses potential future work.</li> </ul> <p>Each section contains detailed explanations and diagrams where appropriate.</p> <p>We hope this documentation provides a clear insight into the proposed system design.</p>"},{"location":"architecture/system_overview/","title":"System Overview","text":"<p>This section provides a high-level overview of the proposed architecture for the job scraping and processing system. The design emphasizes modularity, scalability, and decoupling through an event-driven approach.</p> Component Primary Function Config DB (PostgreSQL) Stores scraping targets, settings, site metadata Django Admin UI for managing the Config DB Dispatcher Service Reads config, creates &amp; queues job messages Message Queue Decouples Dispatcher from Workers, holds jobs Celery Workers Execute scraping tasks (fetch, interact) S3 Bucket Stores raw fetched HTML Parsing Service/Worker Extracts structured data from raw HTML Structured DB (PostgreSQL) Stores cleaned, structured job data Monitoring &amp; Logging Collects logs &amp; metrics for observability Analysis/ML Environment Consumes structured data for analysis"},{"location":"architecture/system_overview/#architecture-diagram","title":"Architecture Diagram","text":"<p>This section provides a high-level overview of the proposed architecture for the job scraping and processing system. To understand the system clearly, we'll look at it from two perspectives, inspired by the C4 model: System Context (Level 1) and focused Container views (Level 2).</p>"},{"location":"architecture/system_overview/#level-1-system-context-diagram","title":"Level 1: System Context Diagram","text":"<p>This diagram shows the Job Scraping System as a single \"black box\" and illustrates how it interacts with its users and the external systems it depends on.</p> <pre><code>graph TD\n    subgraph External Actors &amp; Systems\n        A(Admin User)\n        TWS[Target Websites]\n        AMS[Analysis/ML Systems]\n    end\n\n    subgraph Our System\n        S[Job Scraping System]\n    end\n\n    A -- Manages Configuration --&gt; S;\n    S -- Scrapes Job Data From --&gt; TWS;\n    S -- Provides Cleaned Data To --&gt; AMS;\n\n    style S fill:#lightblue,stroke:#333,stroke-width:2px```</code></pre> <p>Key Interactions (Context):</p> <ol> <li>Admin User: Configures scraping targets, manages settings, monitors system health.</li> <li>Job Scraping System: The complete system.</li> <li>Target Websites: External sites providing job data.</li> <li>Analysis/ML Systems: Downstream consumers of the cleaned data.</li> </ol>"},{"location":"architecture/system_overview/#level-2-container-diagrams-focused-views","title":"Level 2: Container Diagrams (Focused Views)","text":"<p>Instead of a single complex diagram, we break down the system's internal containers and interactions by focusing on one major subsystem at a time.</p>"},{"location":"architecture/system_overview/#1-configuration-subsystem-focus","title":"1. Configuration Subsystem Focus","text":"<p>This view shows how configuration is managed and accessed.</p> <pre><code>graph TD\n    %% Actors\n    A(Admin User)\n\n    %% Focused Subsystem\n    subgraph Configuration\n        B(Config Service / Django App);\n        C{Config DB / PostgreSQL};\n        style C fill:#9cf,stroke:#333,stroke-width:2px;\n    end\n\n    %% Interacting Systems/Containers\n    D(Dispatcher Service)\n\n    %% Interactions\n    A -- Manages via UI --&gt; B;\n    B -- Reads/Writes --&gt; C;\n    D -- Reads Config --&gt; C;</code></pre> <p>Focus: The Admin User interacts with the Config Service (Django App) to manage settings stored in the Config DB. The Dispatcher Service reads from the Config DB to know what jobs to schedule.</p>"},{"location":"architecture/system_overview/#2-job-dispatching-subsystem-focus","title":"2. Job Dispatching Subsystem Focus","text":"<p>This view shows how jobs are created and queued.</p> <pre><code>graph TD\n    %% Focused Subsystem\n    subgraph Dispatching &amp; Queueing\n        D(Dispatcher Service);\n        E{Message Queue};\n        style E fill:#f9f,stroke:#333,stroke-width:2px;\n    end\n\n    %% Interacting Systems/Containers\n    C{Config DB / PostgreSQL};\n    F(Scraping Worker Pool);\n    L(Monitoring System);\n\n    %% Interactions\n    D -- Reads Config --&gt; C;\n    D -- Publishes Job --&gt; E;\n    F -- Consumes Job --&gt; E;\n    D -- Sends Logs/Metrics --&gt; L;</code></pre> <p>Focus: The Dispatcher Service reads from the Config DB, creates job messages, and publishes them to the Message Queue. The Scraping Worker Pool consumes jobs from the queue. The dispatcher also sends operational data to the Monitoring System.</p>"},{"location":"architecture/system_overview/#3-scraping-execution-subsystem-focus","title":"3. Scraping Execution Subsystem Focus","text":"<p>This view shows how scraping tasks are executed and raw data is stored.</p> <pre><code>graph TD\n    %% Focused Subsystem\n    subgraph Scraping Execution\n        F(Scraping Worker Pool / Celery);\n        H(Raw HTML Storage / S3);\n        style H fill:#ccf,stroke:#333,stroke-width:2px;\n    end\n\n    %% Interacting Systems/Containers\n    E{Message Queue};\n    TWS[Target Websites];\n    L(Monitoring System);\n\n    %% Interactions\n    F -- Consumes Job --&gt; E;\n    F -- Fetches From --&gt; TWS;\n    F -- Writes Raw HTML --&gt; H;\n    F -- Sends Logs/Metrics --&gt; L;</code></pre> <p>Focus: The Scraping Worker Pool consumes jobs from the Message Queue, interacts with external Target Websites, stores the fetched raw HTML in Raw HTML Storage, and sends operational data to the Monitoring System.</p>"},{"location":"architecture/system_overview/#4-data-processing-subsystem-focus","title":"4. Data Processing Subsystem Focus","text":"<p>This view shows how raw data is processed and stored structurally.</p> <pre><code>graph TD\n    %% Focused Subsystem\n    subgraph Data Processing\n        I(Parsing Service / Celery or Lambda);\n        J{Structured Data Store / PostgreSQL};\n        style J fill:#9cf,stroke:#333,stroke-width:2px;\n    end\n\n    %% Interacting Systems/Containers\n    H(Raw HTML Storage / S3);\n    AMS[Analysis/ML Systems];\n    L(Monitoring System);\n\n    %% Interactions\n    I -- Reads Raw HTML --&gt; H;\n    I -- Writes Structured Data --&gt; J;\n    AMS -- Reads Clean Data --&gt; J;\n    I -- Sends Logs/Metrics --&gt; L;</code></pre> <p>Focus: The Parsing Service reads raw HTML from Raw HTML Storage, processes it, and writes the cleaned, structured data into the Structured Data Store. Downstream Analysis/ML Systems read from this store. The service also reports to the Monitoring System.</p>"},{"location":"architecture/system_overview/#5-monitoring-analysis-subsystem-focus","title":"5. Monitoring &amp; Analysis Subsystem Focus","text":"<p>This view shows how monitoring data is collected and how final data is consumed.</p> <pre><code>graph TD\n    %% Focused Subsystem\n    subgraph Observability &amp; Consumption\n        L(Monitoring System);\n        J{Structured Data Store / PostgreSQL};\n        style J fill:#9cf,stroke:#333,stroke-width:2px;\n    end\n\n    %% Interacting Systems/Containers\n    A(Admin User)\n    F(Scraping Worker Pool);\n    I(Parsing Service);\n    D(Dispatcher Service);\n    AMS[Analysis/ML Systems];\n\n    %% Interactions\n    F -- Sends Logs/Metrics --&gt; L;\n    I -- Sends Logs/Metrics --&gt; L;\n    D -- Sends Logs/Metrics --&gt; L;\n    %% B(Config Service) -- Sends Logs/Metrics --&gt; L; \n\n    L -- Provides Dashboards/Alerts --&gt; A;\n    AMS -- Reads Clean Data --&gt; J;</code></pre> <p>Focus: Various system components (Workers, Parsing, Dispatcher, etc.) send logs and metrics to the central Monitoring System. The Admin User views dashboards and receives alerts from Monitoring. Analysis/ML Systems consume the final data from the Structured Data Store.</p>"},{"location":"architecture/tech_stack/","title":"Technology Stack Summary","text":"<p>This section outlines the proposed core technologies for each major component of the system, along with justifications based on the design goals.</p> Component Technology Choices Justification Language Python 3.x Mature ecosystem, excellent libraries for web scraping, data processing, web frameworks, ML, and cloud integration. Configuration Mgmt Django + PostgreSQL Django: Built-in Admin UI for easy config management, robust ORM, migrations. PostgreSQL: Reliable relational DB for storing config. Job Scheduling/Dispatch Python Service (potentially Django Mgmt Command/Celery Beat) Leverages Python ecosystem. Can be scheduled via cron, systemd timer, or Celery Beat integrated with the Django app. Message Queue RabbitMQ / Redis / AWS SQS / Google Pub/Sub Decouples dispatcher &amp; workers. Choice depends on scale/ops preference: RabbitMQ: Feature-rich. Redis: Simpler. SQS/PubSub: Managed cloud services. Scraping Workers Celery + Python Celery: Mature, distributed task queue system for Python, integrates well with brokers. Core Fetching Logic <code>requests</code> + <code>urllib3.Retry</code> / <code>backoff</code> Lightweight, standard Python HTTP library with robust, configurable retry/backoff mechanisms for simple fetches. Dynamic Content Fetching Playwright (or potential alternative: Splash/Commercial API) Playwright: Modern browser automation for JS-heavy sites. Alternatives: Considered for resource optimization/offloading browser mgmt. Proxy/UA Rotation Custom Scrapy Middleware / Python Logic in Worker Integrates rotation logic within the fetching process. Can use lists, files, or proxy service APIs. Raw Data Storage AWS S3 / Google Cloud Storage / Azure Blob Storage Scalable, durable, cost-effective object storage for raw HTML. Cloud provider choice often aligns with other infrastructure. Parsing Logic Python (<code>BeautifulSoup</code>, <code>lxml</code>, potentially <code>parsel</code>) Standard Python libraries for efficient HTML parsing. Parsing Service/Worker Celery Workers / AWS Lambda / Google Cloud Functions Celery: Reuse worker infrastructure. Serverless: Option if parsing is stateless and triggered by S3 events. Structured Data Storage PostgreSQL (with JSONB) PostgreSQL: Strong relational features, SQL querying, data integrity. JSONB: Flexibility for semi-structured fields (skills, salary). Monitoring - Logs ELK Stack / Grafana Loki / CloudWatch Logs Centralized log aggregation for debugging and tracing. Monitoring - Metrics Prometheus + Grafana / CloudWatch Metrics / Datadog Time-series metrics for performance monitoring, dashboarding, and alerting. Orchestration (Scale) Docker + Kubernetes (EKS/GKE/AKS) Docker: Containerization. Kubernetes: Scalable deployment, management, auto-scaling of workers and services. Infrastructure Provisioning Terraform / Pulumi / CloudFormation Infrastructure as Code for reproducible and automated environment setup. CI/CD GitHub Actions / GitLab CI / Jenkins Automated building, testing, and deployment pipelines. <p>This stack provides a balance of established, well-supported technologies with scalable cloud-native patterns, aligning with the system's design goals. Specific choices (e.g., RabbitMQ vs. SQS) may be influenced by existing infrastructure or operational preferences.</p>"},{"location":"components/configuration/","title":"Configuration Management (Django)","text":"<p>Effective management of scraping targets, keywords, and settings is crucial for controlling the system's behavior and adapting to new requirements.</p>"},{"location":"components/configuration/#role-purpose","title":"Role &amp; Purpose","text":"<ul> <li>Acts as the central repository for all operational parameters guiding the scraping process.</li> <li>Provides an interface for users/administrators to define what to scrape, where to scrape it from, and how.</li> <li>Stores metadata about target websites needed for successful scraping (e.g., requires Playwright, specific selectors, date filter parameters).</li> </ul>"},{"location":"components/configuration/#technology-choice-django-postgresql","title":"Technology Choice: Django + PostgreSQL","text":"<ul> <li>Django Framework: Chosen primarily for its outstanding built-in Admin Interface. This automatically generates a web UI for managing database models, drastically reducing the effort needed to build a configuration tool.</li> <li>PostgreSQL Database: Serves as the backend database storing the configuration data. Chosen for reliability, robustness, and compatibility with Django.</li> </ul> <pre><code>classDiagram\n    class Website {\n        +int id PK\n        +string name\n        +string base_url\n        +string search_url_template\n        +bool requires_playwright\n        +string pagination_type\n        +bool is_active\n        +float base_delay\n        +json selectors (nullable)\n        +scrape_targets : list~ScrapeTarget~\n    }\n\n    class Keyword {\n        +int id PK\n        +string text\n        +string type (nullable)\n        +scrape_targets : list~ScrapeTarget~\n    }\n\n    class Location {\n        +int id PK\n        +string text\n        +scrape_targets : list~ScrapeTarget~\n    }\n\n    class ScrapeTarget {\n        +int id PK\n        +bool is_active\n        +string frequency\n        +datetime last_scheduled (nullable)\n        +website : Website\n        +keyword : Keyword\n        +location : Location\n        # Represents the specific combination to scrape\n        # Implicit FKs: website_id, keyword_id, location_id\n    }\n\n    Website \"1\" -- \"*\" ScrapeTarget : defines &gt;\n    Keyword \"1\" -- \"*\" ScrapeTarget : uses &gt;\n    Location \"1\" -- \"*\" ScrapeTarget : specifies &gt;</code></pre>"},{"location":"components/configuration/#key-data-models-conceptual","title":"Key Data Models (Conceptual)","text":"<p>The Django application would define models roughly corresponding to:</p>"},{"location":"components/configuration/#website-model","title":"<code>Website</code> Model","text":"<ul> <li><code>name</code>: Human-readable name (e.g., \"LinkedIn Jobs\").</li> <li><code>base_url</code>: Base URL of the site.</li> <li><code>search_url_template</code>: URL structure for performing searches (with placeholders for keywords, location, etc.).</li> <li><code>robots_txt_status</code>: (e.g., Allowed, Disallowed, Not Checked).</li> <li><code>requires_playwright</code>: Boolean flag indicating if browser automation is needed.</li> <li><code>pagination_type</code>: (e.g., 'next_link', 'load_more_button', 'infinite_scroll', 'api').</li> <li><code>date_filter_param</code>: Specific URL parameter or interaction logic for date filtering.</li> <li><code>selectors</code>: (Potentially JSON/TextField) Storing CSS/XPath selectors for key elements (job links, title, description - though parsing logic might be separate).</li> <li><code>is_active</code>: Boolean flag to easily enable/disable scraping for this site.</li> <li><code>base_delay</code>: Default download delay for this site.</li> <li>... other site-specific metadata.</li> </ul>"},{"location":"components/configuration/#keyword-model","title":"<code>Keyword</code> Model","text":"<ul> <li><code>text</code>: The keyword/phrase to search for (e.g., \"Data Engineer\").</li> <li><code>type</code>: (Optional) Category like 'Role', 'Skill', 'Tool'.</li> </ul>"},{"location":"components/configuration/#location-model","title":"<code>Location</code> Model","text":"<ul> <li><code>text</code>: The location to search within (e.g., \"London, UK\", \"Remote\").</li> </ul>"},{"location":"components/configuration/#scrapetarget-model-linking-table","title":"<code>ScrapeTarget</code> Model (Linking Table)","text":"<ul> <li><code>website</code>: Foreign Key to <code>Website</code>.</li> <li><code>keyword</code>: Foreign Key to <code>Keyword</code>.</li> <li><code>location</code>: Foreign Key to <code>Location</code>.</li> <li><code>is_active</code>: Boolean flag for this specific combination.</li> <li><code>frequency</code>: How often to schedule this target (e.g., 'daily', 'hourly').</li> <li><code>last_scheduled</code>: Timestamp of the last dispatch.</li> </ul>"},{"location":"components/configuration/#benefits-of-this-approach","title":"Benefits of this Approach","text":"<ul> <li>Centralized Control: All configuration in one place.</li> <li>User-Friendly Management: Easy updates via the Django Admin UI without direct database access or code changes for simple config updates.</li> <li>Structured &amp; Relational: Database enforces relationships (e.g., which keywords apply to which sites).</li> <li>Version Control: Django migrations allow schema changes to be version controlled.</li> <li>Programmatic Access: The Dispatcher service can easily query these models using Django's ORM.</li> </ul>"},{"location":"components/dispatcher/","title":"Dispatcher Service","text":"<p>The Dispatcher acts as the brain coordinating the scraping activities based on the central configuration.</p> <pre><code>sequenceDiagram\n    participant S as Scheduler\n    participant D as Dispatcher\n    participant C as Config DB\n    participant Q as Message Queue\n\n    loop Check Schedule\n        S-&gt;&gt;D: Trigger Dispatch Check\n        D-&gt;&gt;C: Query Due ScrapeTargets\n        C--&gt;&gt;D: Return Due Targets\n        alt For Each Due Target\n            D-&gt;&gt;D: Construct Job Message\n            D-&gt;&gt;Q: Publish Job Message\n            D-&gt;&gt;C: Update last_scheduled\n        end\n    end</code></pre>"},{"location":"components/dispatcher/#role-purpose","title":"Role &amp; Purpose","text":"<ul> <li>Periodically queries the Configuration Database (Django/PostgreSQL) to identify active <code>ScrapeTarget</code> combinations that are due for execution based on their defined <code>frequency</code> and <code>last_scheduled</code> time.</li> <li>Constructs specific job instructions for each due target.</li> <li>Formats these instructions into messages suitable for the Message Queue.</li> <li>Publishes these job messages to the Message Queue for consumption by the Scraper Workers.</li> <li>Updates the <code>last_scheduled</code> timestamp in the Configuration Database for dispatched targets.</li> </ul>"},{"location":"components/dispatcher/#implementation-options","title":"Implementation Options","text":"<ul> <li>Standalone Python Service: A script running continuously or triggered periodically (e.g., via <code>cron</code> or a systemd timer). Uses a library like <code>SQLAlchemy</code> or Django's ORM (if run within Django context) to query the config DB and a library like <code>pika</code> (RabbitMQ), <code>boto3</code> (SQS), or <code>redis-py</code> to publish messages.</li> <li>Django Management Command: A command integrated within the Django application, scheduled using <code>cron</code> or similar. Can directly use Django's ORM.</li> <li>Celery Beat Task: Utilize Celery's periodic task scheduler (<code>Celery Beat</code>) to run the dispatch logic as a recurring task within the Celery ecosystem, especially if Celery is already heavily used. Can directly use Django's ORM if configured.</li> <li>Serverless Function (e.g., AWS Lambda, Google Cloud Function): Triggered on a schedule (e.g., CloudWatch Events). Queries the database (requires network access/credentials) and publishes to the queue.</li> </ul>"},{"location":"components/dispatcher/#job-message-content","title":"Job Message Content","text":"<p>Each message published to the queue needs to contain sufficient information for a worker to execute the scrape. Example contents:</p> <pre><code>{\n  \"target_id\": 123, // ID from ScrapeTarget table\n  \"website_id\": 5,\n  \"website_name\": \"ExampleJobs.com\",\n  \"search_url\": \"https://www.examplejobs.com/search?q=Python+Developer&amp;loc=Remote&amp;posted=24h\",\n  \"requires_playwright\": false,\n  \"pagination_type\": \"next_link\",\n  // ... other necessary site-specific metadata ...\n  \"keywords_used\": [\"Python Developer\"],\n  \"location_used\": \"Remote\",\n  \"attempt_count\": 0 // Initial attempt\n}\n</code></pre>"},{"location":"components/parsing/","title":"Parsing &amp; Structured Data Extraction","text":"<p>Once raw HTML content is successfully fetched and stored, it needs to be parsed to extract meaningful, structured information.</p>"},{"location":"components/parsing/#role-purpose","title":"Role &amp; Purpose","text":"<ul> <li>Retrieve raw HTML content associated with a completed scraping job (typically from S3).</li> <li>Parse the HTML structure.</li> <li>Identify and extract specific data fields based on predefined rules or selectors (e.g., job title, company name, description, salary text).</li> <li>Perform initial cleaning and standardization of extracted data (e.g., trim whitespace, format dates).</li> <li>Load the structured, cleaned data into the target Structured Data Storage (PostgreSQL).</li> </ul>"},{"location":"components/parsing/#decoupled-approach","title":"Decoupled Approach","text":"<ul> <li>This parsing step is intentionally decoupled from the initial fetching step (scraper workers).</li> <li>Benefits:<ul> <li>Allows fetching and parsing to scale independently.</li> <li>Enables reprocessing of raw HTML from S3 if parsing logic changes, without re-scraping websites.</li> <li>Parsing might have different resource requirements (more CPU-bound for parsing, less I/O-bound compared to fetching).</li> </ul> </li> </ul>"},{"location":"components/parsing/#implementation-options","title":"Implementation Options","text":""},{"location":"components/parsing/#option-a-dedicated-parsing-workers-celery","title":"Option A: Dedicated Parsing Workers (Celery)","text":"<pre><code>graph TD\n    A(Receive Parsing Job Message S3 Path) --&gt; B(Download HTML from S3);\n    B --&gt; C(Decompress HTML);\n    C --&gt; D(Parse HTML);\n    D --&gt; E(Extract &amp; Clean Fields);\n    E --&gt; F(Standardize Data);\n    F --&gt; G{Validate Data Quality};\n    G -- OK --&gt; H[Insert/Update Structured DB];\n    H --&gt; I[Acknowledge Queue Message];\n    G -- Failed --&gt; J[Log Validation Error];\n    J --&gt; I;\n\n    style I fill:#cfc,stroke:#333,stroke-width:2px</code></pre> <ul> <li>Workflow:<ol> <li>Scraper worker successfully fetches HTML and uploads to S3.</li> <li>Scraper worker (or an S3 event trigger) publishes a \"parsing job\" message to a separate Message Queue, including the S3 path of the raw HTML.</li> <li>A dedicated pool of Celery workers consumes these parsing messages.</li> <li>The parsing worker downloads the HTML from S3, decompresses it, parses it, and inserts the structured data into PostgreSQL.</li> </ol> </li> <li>Pros: Clear separation of concerns, independent scaling of parsing workers.</li> <li>Cons: Requires an additional queue and message flow.</li> </ul>"},{"location":"components/parsing/#option-b-serverless-functions-eg-aws-lambda","title":"Option B: Serverless Functions (e.g., AWS Lambda)","text":"<ul> <li>Workflow:<ol> <li>Scraper worker uploads raw HTML to S3.</li> <li>Configure an S3 event notification to trigger a Lambda function (or Google Cloud Function/Azure Function) whenever a new HTML file is created.</li> <li>The serverless function receives the event (containing S3 path), downloads the HTML, parses it, and inserts data into PostgreSQL (requires appropriate permissions and network configuration).</li> </ol> </li> <li>Pros: Potentially simpler infrastructure management (no parsing workers to manage), scales automatically based on S3 events.</li> <li>Cons: Limited execution time/memory (usually configurable but has limits), potential cold starts, managing database connections efficiently from serverless functions requires care.</li> </ul>"},{"location":"components/parsing/#option-c-integrated-parsing-less-recommended-for-decoupling","title":"Option C: Integrated Parsing (Less Recommended for Decoupling)","text":"<ul> <li>Workflow: The same scraper worker that fetches the HTML immediately parses it before finishing the task.</li> <li>Pros: Simpler flow, fewer components.</li> <li>Cons: Tightly couples fetching and parsing; loses the ability to re-parse from S3 independently; workers become heavier; harder to scale fetching and parsing independently.</li> </ul> <p>Recommended Approach: Option A (Dedicated Parsing Workers) or Option B (Serverless Functions) are generally preferred for better decoupling and scalability over Option C. The choice between A and B depends on operational preference and existing infrastructure.</p>"},{"location":"components/parsing/#parsing-libraries","title":"Parsing Libraries","text":"<ul> <li>Python Libraries: Use standard, efficient libraries like:<ul> <li><code>BeautifulSoup4</code>: Flexible and forgiving parser, easy to use.</li> <li><code>lxml</code>: Very fast XML/HTML parser, often used with <code>BeautifulSoup</code> or directly via its <code>etree</code> interface.</li> <li><code>parsel</code>: The selector library used by Scrapy, can be used standalone, supports CSS and XPath selectors well.</li> </ul> </li> </ul>"},{"location":"components/parsing/#selector-management","title":"Selector Management","text":"<ul> <li>Parsing logic often relies on CSS selectors or XPath expressions to locate data. These can be brittle if website structure changes.</li> <li>Store selectors potentially in the Configuration Database alongside website metadata, allowing easier updates without code deployments for simple selector changes.</li> <li>Implement robust error handling in parsing logic to detect when selectors fail.</li> </ul>"},{"location":"components/queue/","title":"Task Queue","text":"<p>The Message Queue is the central communication channel decoupling the Dispatcher from the pool of Scraper Workers.</p>"},{"location":"components/queue/#role-purpose","title":"Role &amp; Purpose","text":"<ul> <li>Acts as a buffer holding job messages sent by the Dispatcher.</li> <li>Distributes job messages to available Scraper Workers.</li> <li>Enhances system resilience: If workers are down, messages wait in the queue. If the dispatcher is down, workers continue processing existing messages.</li> <li>Enables independent scaling of Dispatcher and Workers.</li> </ul> <pre><code>sequenceDiagram\n    participant D as Dispatcher\n    participant Q as Message Queue Broker\n    participant W as Celery Worker\n\n    Note over D, W: Worker is idle, waiting for jobs.\n\n    D-&gt;&gt;+Q: Publish(Job Message)\n    Note over Q: Message stored persistently.\n    Q--&gt;&gt;-D: Publish Acknowledged (Optional)\n\n    Note over Q, W: Broker makes message available.\n    W-&gt;&gt;+Q: Consume()/Get Message\n    Note over Q: Message marked as \"unacknowledged\".\n    Q--&gt;&gt;-W: Job Message\n\n    Note over W: Worker begins processing job...\n    W-&gt;&gt;W: Execute Scraping Task (Fetch, Parse etc.)\n\n    alt Task Successful\n        Note over W: Job completed successfully.\n        W-&gt;&gt;+Q: Acknowledge(Message)\n        Note over Q: Message permanently removed.\n        Q--&gt;&gt;-W: Ack Confirmed (Optional)\n    else Task Failed (After Retries)\n        Note over W: Job failed after all retries.\n        W-&gt;&gt;+Q: Negative Acknowledge / Route to DLQ\n        Note over Q: Message moved to Dead-Letter Queue or discarded.\n        Q--&gt;&gt;-W: Nack/Route Confirmed (Optional)\n    else Worker Crashes Mid-Task\n        Note over W: Worker crashes before acknowledging.\n        Note over Q: Message remains \"unacknowledged\".&lt;br/&gt;After visibility timeout,&lt;br/&gt;Broker makes message available again&lt;br/&gt;for another worker.\n    end</code></pre>"},{"location":"components/queue/#technology-choices","title":"Technology Choices","text":"<ul> <li>RabbitMQ: Feature-rich, mature, protocol-based (AMQP) message broker. Offers routing flexibility, acknowledgements, persistence, and DLQ support. Requires separate deployment and management.</li> <li>Redis: Often used as a simpler message broker for Celery. Fast in-memory store, but persistence and reliability features might be less robust than RabbitMQ depending on configuration. Can serve multiple purposes (caching, broker).</li> <li>AWS SQS (Simple Queue Service): Fully managed cloud service. Highly scalable and durable. Offers standard and FIFO queues, DLQ support. Reduces operational overhead. Pay-per-use pricing.</li> <li>Google Cloud Pub/Sub: Fully managed cloud service. Global scale, push/pull delivery, filtering. Reduces operational overhead.</li> </ul> <p>Selection Criteria: Choice depends on factors like existing infrastructure, operational preferences (managed vs. self-hosted), required features (e.g., message ordering guarantees - usually not needed for scraping jobs), and scalability needs. Managed services (SQS, Pub/Sub) are often preferred for cloud-native deployments to reduce operational burden.</p>"},{"location":"components/queue/#key-features-used","title":"Key Features Used","text":"<ul> <li>Message Persistence: Ensure messages are not lost if the broker restarts.</li> <li>Worker Acknowledgements: Workers should acknowledge messages only after successfully completing the task (or deciding it's a terminal failure) to prevent message loss if a worker crashes mid-task.</li> <li>Dead-Letter Queue (DLQ) Support: Automatically route messages that fail repeatedly to a separate queue for investigation.</li> <li>Scalability: The queue system must handle the peak rate of job dispatch and consumption.</li> </ul>"},{"location":"components/queue/#interaction-flow","title":"Interaction Flow","text":"<ol> <li>Dispatcher publishes a job message to a specific queue/topic.</li> <li>The Message Broker makes the message available.</li> <li>An available Celery Worker retrieves (consumes) the message.</li> <li>The Worker processes the job.</li> <li>Upon completion/failure, the Worker sends an acknowledgement (<code>ack</code>) to the Broker, removing the message from the queue (or routing to DLQ upon repeated failure).</li> </ol>"},{"location":"components/structured_storage/","title":"Structured Data Storage (PostgreSQL)","text":"<p>This component is the destination for the cleaned, processed, and structured job posting data extracted from the raw HTML.</p>"},{"location":"components/structured_storage/#role-purpose","title":"Role &amp; Purpose","text":"<ul> <li>Store the final, usable job data in a structured format suitable for querying and analysis.</li> <li>Enforce data types and potentially basic constraints (e.g., uniqueness on source URL + timestamp).</li> <li>Serve as the primary data source for downstream applications, analytics platforms, or ML model training pipelines.</li> </ul>"},{"location":"components/structured_storage/#technology-choice-postgresql","title":"Technology Choice: PostgreSQL","text":"<ul> <li>Rationale:<ul> <li>Relational Integrity: Strong support for data types, constraints, and relationships.</li> <li>Powerful SQL: Standard, expressive query language for complex data retrieval and aggregation.</li> <li>JSONB Support: Excellent built-in support for storing and efficiently querying semi-structured JSON data within relational tables. This is ideal for handling fields with variable structures (e.g., salary details, extracted skills lists, location breakdowns).</li> <li>Maturity &amp; Reliability: Proven, ACID-compliant database with a large ecosystem and community support.</li> <li>Scalability: Scales well vertically and offers various replication and high-availability options. Extensions like <code>pgvector</code> can add specialized capabilities if needed later.</li> </ul> </li> </ul>"},{"location":"components/structured_storage/#data-schema-design-conceptual","title":"Data Schema Design (Conceptual)","text":"<p>A primary table (e.g., <code>JobPostings</code>) would store the core information.</p>"},{"location":"components/structured_storage/#jobpostings-table-example","title":"<code>JobPostings</code> Table Example","text":"<ul> <li><code>id</code>: Primary Key (Serial).</li> <li><code>source_website</code>: Text (e.g., 'linkedin', 'indeed_com').</li> <li><code>source_url</code>: Text (Unique identifier for the specific posting).</li> <li><code>job_id_on_source</code>: Text (Nullable, ID used by the source site).</li> <li><code>scraped_timestamp</code>: TimestampTZ (When this record was scraped/parsed).</li> <li><code>first_seen_timestamp</code>: TimestampTZ (When this job URL was first encountered).</li> <li><code>last_processed_timestamp</code>: TimestampTZ (When this record was last updated by the parser).</li> <li><code>raw_html_s3_path</code>: Text (Link back to the raw source in S3).</li> <li><code>job_title_raw</code>: Text.</li> <li><code>company_name</code>: Text (Nullable).</li> <li><code>location_raw</code>: Text (Nullable).</li> <li><code>location_structured</code>: JSONB (Nullable, e.g., <code>{\"city\": \"London\", \"country\": \"UK\", \"remote\": false}</code>).</li> <li><code>description_text</code>: Text (Cleaned job description).</li> <li><code>description_html</code>: Text (Nullable, Raw HTML description if needed).</li> <li><code>publish_date</code>: Date/TimestampTZ (Nullable, Standardized).</li> <li><code>employment_type</code>: Text (Nullable, e.g., 'Full-time', 'Contract').</li> <li><code>salary_raw</code>: Text (Nullable, The raw salary string).</li> <li><code>salary_structured</code>: JSONB (Nullable, e.g., <code>{\"min\": 50000, \"max\": 70000, \"currency\": \"USD\", \"period\": \"annual\"}</code>).</li> <li><code>extracted_skills</code>: JSONB (Nullable, Array of strings, e.g., <code>[\"Python\", \"SQL\", \"AWS\"]</code>).</li> <li><code>categorized_role</code>: Text (Nullable, Standardized role category).</li> <li><code>seniority_level</code>: Text (Nullable, Inferred seniority).</li> <li>... other relevant fields.</li> </ul>"},{"location":"components/structured_storage/#indexing","title":"Indexing","text":"<ul> <li>Create indexes on frequently queried columns like <code>source_url</code>, <code>scraped_timestamp</code>, <code>source_website</code>, <code>job_title_raw</code>, <code>company_name</code>, and potentially use GIN indexes for efficient querying within <code>JSONB</code> fields.</li> </ul>"},{"location":"components/structured_storage/#data-interaction","title":"Data Interaction","text":"<ul> <li>The Parsing Service/Worker is responsible for <code>INSERT</code>ing new records or potentially <code>UPDATE</code>ing existing ones (based on <code>source_url</code>) if re-processing occurs.</li> <li>Downstream analysis tools, ML pipelines, or applications query this database using SQL.</li> </ul> <p>This setup provides a robust and flexible foundation for storing and accessing the valuable structured job data.</p>"},{"location":"components/workers/","title":"Scraper Workers (Celery)","text":"<p>The Workers are the core execution units responsible for performing the actual web scraping tasks.</p> <pre><code>graph TD\n    A[Receive Job Message] --&gt; B{Parse Message};\n    B --&gt; C{Requires Playwright?};\n    C -- Yes --&gt; D[Run Playwright Fetch Logic];\n    C -- No --&gt; E[Run Requests Fetch Logic];\n    D --&gt; F{Fetch Success?};\n    E --&gt; F;\n    F -- Yes --&gt; G[Compress HTML];\n    G --&gt; H[Upload HTML to S3];\n    H --&gt; I[Log Success / Report Stats];\n    I --&gt; J[Acknowledge Queue Message];\n    F -- No (After Retries) --&gt; K[Log Error / Report Stats];\n    K --&gt; J;\n\n    style J fill:#cfc,stroke:#333,stroke-width:2px</code></pre>"},{"location":"components/workers/#role-purpose","title":"Role &amp; Purpose","text":"<ul> <li>Consume job messages from the Message Queue.</li> <li>Interpret the job message to understand the target URL and required actions/parameters.</li> <li>Execute the data fetching logic using either lightweight HTTP clients (<code>requests</code>) or browser automation (<code>Playwright</code>) as specified.</li> <li>Implement politeness delays, proxy rotation, and User-Agent rotation during fetching.</li> <li>Handle fetcher-level errors and retries.</li> <li>Process pagination or dynamic content loading logic.</li> <li>Upon successful fetching, store the raw HTML content into the S3 Bucket.</li> <li>Report task status (success, failure, retry needed) back via Celery and potentially to the Monitoring system.</li> <li>Acknowledge messages from the queue upon definitive completion or failure.</li> </ul>"},{"location":"components/workers/#technology-choice-celery","title":"Technology Choice: Celery","text":"<ul> <li>Rationale: Celery is the standard, feature-rich distributed task queue framework for Python.</li> <li>Benefits:<ul> <li>Integrates seamlessly with message brokers (RabbitMQ, Redis, SQS).</li> <li>Handles worker management, task distribution, and concurrency.</li> <li>Provides built-in support for task retries with backoff.</li> <li>Mature and well-documented.</li> </ul> </li> </ul>"},{"location":"components/workers/#worker-implementation-details","title":"Worker Implementation Details","text":""},{"location":"components/workers/#task-definition","title":"Task Definition","text":"<ul> <li>Define Celery tasks corresponding to the scraping jobs (e.g., a single task type that adapts based on message parameters).</li> <li>The task receives the job message content as input.</li> </ul>"},{"location":"components/workers/#fetching-logic-integration","title":"Fetching Logic Integration","text":"<ul> <li>The task code imports and uses the chosen fetching libraries (<code>requests</code> with <code>Retry</code> adapter, <code>Playwright</code>).</li> <li>Logic branches based on <code>requires_playwright</code> flag in the job message.</li> </ul>"},{"location":"components/workers/#resource-management","title":"Resource Management","text":"<ul> <li>Concurrency: Configure the number of concurrent tasks each Celery worker process can handle (<code>--concurrency</code> flag). This needs careful tuning, especially when using Playwright, to avoid overwhelming CPU/RAM. Consider using <code>gevent</code> or <code>eventlet</code> execution pools for I/O-bound <code>requests</code> tasks, but stick to <code>prefork</code> (default) or <code>solo</code> for CPU/RAM-intensive Playwright tasks.</li> <li>Playwright Cleanup: Ensure Playwright browser instances and contexts are closed properly at the end of each task (or reused carefully) to prevent resource leaks.</li> <li>Containerization: Run workers inside Docker containers managed by Kubernetes for scaling and resource isolation. Define appropriate CPU/Memory requests and limits in Kubernetes deployments.</li> </ul>"},{"location":"components/workers/#scaling","title":"Scaling","text":"<ul> <li>Scale the number of worker instances (pods in Kubernetes) horizontally based on the depth of the Message Queue or CPU/Memory load using Kubernetes HPA and potentially KEDA for queue-based scaling.</li> </ul>"},{"location":"components/workers/#error-handling-within-workers","title":"Error Handling within Workers","text":"<ul> <li>Implement <code>try...except</code> blocks around fetching and processing logic.</li> <li>Utilize Celery's <code>task.retry()</code> for recoverable errors.</li> <li>Log all errors comprehensively to the central logging system.</li> <li>Ensure tasks eventually terminate (success or failure) and acknowledge the message queue correctly.</li> </ul>"},{"location":"conclusion/future_improvements/","title":"Future Considerations &amp; Potential Improvements","text":"<p>While the proposed design provides a robust foundation, several areas could be explored for future enhancement or optimization as the system matures and requirements evolve:</p>"},{"location":"conclusion/future_improvements/#scraping-anti-scraping-enhancements","title":"Scraping &amp; Anti-Scraping Enhancements","text":"<ul> <li>Implement more sophisticated browser fingerprinting evasion if targeting highly protected sites.</li> <li>Explore AI/ML techniques for detecting website structure changes or identifying relevant data fields more dynamically.</li> <li>Integrate dedicated CAPTCHA-solving services if manual rotation/avoidance proves insufficient for key targets.</li> </ul>"},{"location":"conclusion/future_improvements/#data-processing-ml-integration","title":"Data Processing &amp; ML Integration","text":"<ul> <li>Develop and integrate more sophisticated Machine Learning models directly within the processing pipeline for extracting skills, categorizing roles, inferring seniority, or detecting salary information more accurately from job descriptions.</li> <li>Implement more formal data quality validation rules and automated reporting on data quality metrics.</li> </ul>"},{"location":"conclusion/future_improvements/#analytics-data-storage-evolution","title":"Analytics &amp; Data Storage Evolution","text":"<ul> <li>If analytical query performance over very large historical datasets becomes critical, establish an ETL/ELT process to load structured data into a dedicated Data Warehouse (e.g., Snowflake, BigQuery, Redshift) optimized for OLAP workloads.</li> <li>For large-scale ML training or cheaper long-term storage of processed data, consider utilizing a Data Lake architecture (e.g., storing data as Parquet files on S3 queried via Spark/Athena/Presto).</li> <li>Implement a dedicated Vector Database or leverage <code>pgvector</code> more extensively if semantic search or similarity analysis based on embeddings becomes a core requirement.</li> </ul>"},{"location":"conclusion/future_improvements/#operational-ui-improvements","title":"Operational &amp; UI Improvements","text":"<ul> <li>Develop a more sophisticated user interface beyond the basic Django Admin for configuration management, job monitoring, and viewing results.</li> <li>Implement more granular cost monitoring and reporting tied to specific scraping activities or sites.</li> <li>Automate responses to certain alerts, such as temporarily pausing dispatch for sites exhibiting persistent high failure rates.</li> </ul>"},{"location":"conclusion/future_improvements/#scope-expansion","title":"Scope Expansion","text":"<ul> <li>Extend the framework to handle scraping of other relevant data types beyond job listings, such as company profiles, salary comparison data, or industry news.</li> </ul> <p>These potential improvements represent directions for future development, building upon the scalable and maintainable core architecture proposed in this document. The initial focus remains on implementing the core system effectively to meet the primary challenge requirements.</p>"},{"location":"conclusion/summary/","title":"Summary of Design","text":"<p>This document has outlined a proposed architecture for a scalable, reliable, and maintainable system designed to scrape, process, and store publicly available job listing data.</p>"},{"location":"conclusion/summary/#core-problem-addressed","title":"Core Problem Addressed","text":"<p>The system tackles the challenge of collecting diverse job postings from numerous websites, across various sectors, locations, and roles, while handling the complexities inherent in web scraping, such as dynamic content, pagination, rate limits, and anti-scraping measures.</p>"},{"location":"conclusion/summary/#key-architectural-pillars","title":"Key Architectural Pillars","text":"<ul> <li>Event-Driven &amp; Decoupled: Utilizes a message queue (e.g., RabbitMQ/SQS) to decouple job dispatching from execution, enhancing resilience and scalability. Distinct components handle specific tasks (configuration, dispatch, fetching, parsing, storage).</li> <li>Scalable Worker Pool: Employs Celery workers, orchestrated potentially via Kubernetes, allowing horizontal scaling to handle varying scraping loads.</li> <li>Hybrid Fetching Strategy: Leverages lightweight <code>requests</code> with robust retries for simpler sites and incorporates <code>Playwright</code> (or alternatives like Splash/APIs) for JavaScript-heavy dynamic sites, managed within the workers.</li> <li>Dual Storage Layers: Uses cloud object storage (S3) for raw, unprocessed HTML (enabling reprocessing and auditing) and a structured relational database (PostgreSQL with JSONB) for cleaned, queryable data suitable for analysis.</li> <li>Centralized Configuration: Employs Django with its built-in Admin UI for easy and accessible management of target websites, keywords, and scraping parameters.</li> <li>Emphasis on Operations: Incorporates comprehensive monitoring (logging, metrics), alerting, robust error handling (retries, DLQs), and standard deployment practices (Docker, K8s, IaC, CI/CD) for maintainability and reliability at scale.</li> </ul>"},{"location":"conclusion/summary/#alignment-with-design-goals","title":"Alignment with Design Goals","text":"<p>This design directly addresses the core goals set out initially:</p> <ul> <li>Scalability: Through worker pools, message queues, and scalable cloud services.</li> <li>Maintainability: Via modular components, centralized configuration, IaC, and CI/CD.</li> <li>Reliability: Through decoupled architecture, comprehensive error handling, retries, and monitoring.</li> <li>Efficiency: By using targeted scraping, appropriate tool selection (requests vs. Playwright), caching considerations, and compression.</li> </ul> <p>In conclusion, the proposed architecture provides a solid foundation for building a powerful job market data collection platform, capable of adapting to new sources and scaling to meet future demands.</p>"},{"location":"introduction/challenge_overview/","title":"Challenge Overview","text":"<p>This document outlines the proposed design for a system addressing the data engineering challenge presented. The core objective is to design and plan the implementation of a scalable web scraping system focused on collecting publicly available job listing data across various sources.</p> <p>Project Focus:</p> <p>The specific application domain for this design is the collection of job postings from:</p> <ul> <li>Multiple job boards (e.g., LinkedIn, Indeed, specific industry boards).</li> <li>Various industry sectors.</li> <li>Different geographic locations/cities.</li> <li>A diverse set of roles and job titles.</li> </ul> <p>Core Tasks Addressed (Based on Exercise 1 &amp; 2):</p> <ol> <li> <p>Web Scraping Strategy &amp; Implementation (Exercise 1):</p> <ul> <li>Define an approach to identify relevant websites and scrape job data efficiently and ethically.</li> <li>Plan for handling common scraping challenges like dynamic content loading, pagination, rate limiting, and anti-scraping measures.</li> <li>Design a data model for storing the collected information (both raw and structured).</li> <li>Address data quality concerns like duplicates and missing values during collection.</li> <li>Select and justify appropriate programming languages, libraries, and frameworks (e.g., Python, Scrapy, Playwright, Requests).</li> <li>Outline storage solutions for raw scraped data (e.g., S3).</li> </ul> </li> <li> <p>Data Processing &amp; Optimization (Exercise 2):</p> <ul> <li>Design a workflow to clean, standardize, and preprocess the scraped data for usability.</li> <li>Plan steps required if the data were to be used for machine learning applications (e.g., text processing, feature extraction).</li> <li>Outline strategies for optimizing the scraping and processing system for large-scale data collection and long-term efficiency (e.g., caching, parallel processing, monitoring).</li> <li>Define storage solutions for processed, structured data suitable for analysis (e.g., PostgreSQL).</li> </ul> </li> </ol> <p>The ultimate goal is to propose a robust, scalable, and maintainable architecture capable of collecting, processing, and storing valuable job market data for further analysis or application development.</p>"},{"location":"introduction/design_goals/","title":"Design Goals &amp; Principles","text":"<p>The design of this job scraping and processing system is guided by the following key principles, ensuring it meets the challenge requirements while adhering to good software and data engineering practices:</p> <ul> <li> <p>Scalability:</p> <ul> <li>The architecture must handle a growing number of target websites, keywords, locations, and roles without significant redesign.</li> <li>It should scale horizontally to accommodate increasing data volume and processing load (e.g., adding more workers, scaling databases and queues).</li> <li>Resource usage should be considered, balancing performance with potential costs.</li> </ul> </li> <li> <p>Maintainability:</p> <ul> <li>Components should be modular and well-defined, allowing for easier updates and bug fixes.</li> <li>Configuration (target sites, keywords, settings) should be managed centrally and easily updatable (e.g., via Django Admin).</li> <li>Code should be clear, well-documented, and potentially testable. This documentation site itself is part of this goal.</li> </ul> </li> <li> <p>Reliability &amp; Resilience:</p> <ul> <li>The system must gracefully handle inevitable failures (network errors, website changes, temporary blocks).</li> <li>Implement robust error handling, logging, and automated retries at appropriate levels (fetching, task execution).</li> <li>Decoupled components (via queues) prevent failures in one part from immediately cascading and halting the entire system.</li> </ul> </li> <li> <p>Decoupling:</p> <ul> <li>Separate distinct concerns: Configuration management, job dispatching, data fetching, data parsing, and data storage should be handled by distinct components or services.</li> <li>This improves maintainability, testability, and allows different parts of the system to scale independently. Queues (like RabbitMQ/SQS) and distinct storage layers (S3, PostgreSQL) are key enablers.</li> </ul> </li> <li> <p>Efficiency:</p> <ul> <li>Optimize scraping processes where possible (e.g., check for APIs before resorting to full browser rendering).</li> <li>Use efficient data formats and storage mechanisms (e.g., HTML compression on S3, appropriate database indexing).</li> <li>Implement targeted scraping using filters (dates, keywords) to avoid unnecessary crawling and reduce load on target sites.</li> </ul> </li> <li> <p>Data Quality &amp; Integrity:</p> <ul> <li>Aim to collect accurate data through careful parsing and validation.</li> <li>Implement processes to handle duplicates, missing values, and inconsistencies during the data processing stage.</li> <li>Store data in a structured, usable format suitable for analysis.</li> </ul> </li> <li> <p>Monitorability:</p> <ul> <li>The system's health and performance must be observable. Implement logging, metrics collection, and potentially alerting to understand throughput, error rates, queue depths, and resource usage.</li> </ul> </li> </ul> <p>These principles guide the specific technology choices and architectural decisions detailed in the subsequent sections of this document.</p>"},{"location":"operations/efficiency/","title":"Long-Term Efficiency","text":"<p>Ensuring the scraping system operates efficiently over the long term involves optimizing resource usage, minimizing redundant work, and reducing costs.</p>"},{"location":"operations/efficiency/#caching-strategies","title":"Caching Strategies","text":"<ul> <li>Purpose: Avoid re-fetching or re-processing data that hasn't changed.</li> <li>HTTP Caching (Conditional):<ul> <li>Respect standard HTTP caching headers (<code>ETag</code>, <code>Last-Modified</code>) sent by target websites if appropriate. The <code>requests</code> library can sometimes handle this partially.</li> <li>Caution: Aggressive client-side HTTP caching can sometimes interfere with detecting fresh content on job boards. Needs careful implementation if used.</li> </ul> </li> <li>Application-Level Caching:<ul> <li>Raw HTML Cache: Before initiating a parse task, check if the raw HTML for a specific URL/version already exists in S3 (or a faster cache like Redis if needed, though S3 is often sufficient).</li> <li>Parsed Data Cache (Less Common): If parsing is extremely expensive and source HTML changes infrequently, potentially cache the structured output, but this adds complexity in cache invalidation.</li> <li>Configuration Cache: Cache frequently accessed configuration details in the dispatcher or workers to reduce load on the config database.</li> </ul> </li> </ul>"},{"location":"operations/efficiency/#incremental-scraping","title":"Incremental Scraping","text":"<ul> <li>Goal: Avoid repeatedly scraping job listings that have already been processed and haven't changed. Focus on new or updated listings.</li> <li>Strategies:<ul> <li>Date Filters: Leverage website features to filter searches by \"posted in last 24 hours\" or \"posted since [date]\". This is the most direct approach if available and reliable. Store the filter logic in the configuration database.</li> <li>Identify New Listings: When scraping search result pages, compare the found Job IDs or URLs against those recently seen/stored in the structured database. Only yield requests for new/unseen detail pages.</li> <li>Detect Updates (Harder): For already scraped jobs, periodically re-scrape detail pages and compare key fields or a hash of the content to detect significant updates. This is more resource-intensive. A <code>last_checked_timestamp</code> in the structured data can help manage this.</li> </ul> </li> </ul>"},{"location":"operations/efficiency/#api-alternatives","title":"API Alternatives","text":"<ul> <li>Reiteration: Before building a scraper for a website, always investigate if a public or private API exists that provides the required job data.</li> <li>Benefits: APIs are generally more stable, efficient, provide structured data (often JSON), and are the intended way to access data programmatically.</li> <li>Process: Use browser developer tools to monitor network requests while interacting with the target site. Look for XHR/Fetch requests that return structured data.</li> </ul>"},{"location":"operations/efficiency/#resource-optimization","title":"Resource Optimization","text":"<ul> <li>Tool Selection: Prefer lightweight tools (<code>requests</code>) over heavyweight ones (Playwright) whenever possible. Use Playwright only when strictly necessary for JS rendering or complex interactions.</li> <li>Efficient Parsing: Use efficient HTML parsing libraries like <code>lxml</code>.</li> <li>Concurrency Tuning: Adjust the number of concurrent workers (Celery) and potentially internal concurrency settings (e.g., Playwright browser contexts) based on monitoring data (CPU/RAM usage, task duration, external site responsiveness) to maximize throughput without overloading resources or getting banned.</li> <li>Instance Sizing: Choose appropriate instance sizes (CPU/RAM) for worker nodes, database instances, and other services based on observed load. Use cloud provider auto-scaling features where applicable (especially K8s HPA).</li> <li>Data Compression: Compress raw HTML stored in S3 using <code>gzip</code> or <code>brotli</code> to reduce storage costs.</li> </ul>"},{"location":"operations/efficiency/#parallel-processing","title":"Parallel Processing","text":"<ul> <li>Mechanism: Primarily achieved through running multiple Celery workers concurrently, potentially distributed across multiple machines managed by Kubernetes.</li> <li>Benefits: Enables high throughput by processing many scraping and parsing tasks simultaneously.</li> <li>Considerations: Ensure downstream resources (databases, APIs) can handle the concurrent load. Manage shared resources like proxy pools effectively across workers.</li> </ul> <p>By focusing on these efficiency measures, the system can minimize redundant work, optimize resource consumption, and operate more cost-effectively at scale.</p>"},{"location":"operations/error_handling/","title":"Error Handling &amp; Retries","text":"<p>A robust scraping system must anticipate and handle various failures gracefully. This section outlines the multi-layered approach to error handling and retries.</p>"},{"location":"operations/error_handling/#fetcher-level-retries","title":"Fetcher-Level Retries","text":"<ul> <li>Mechanism: Implemented directly within the data fetching logic used by the Celery workers.</li> <li>Tools: Utilizing the <code>requests</code> library with <code>urllib3.Retry</code> adapter or the <code>backoff</code> library decorator.</li> <li>Strategy:<ul> <li>Automatically retry requests that fail due to transient network errors (timeouts, connection errors).</li> <li>Automatically retry requests based on specific HTTP status codes indicating temporary server issues (e.g., 500, 502, 503, 504, 429 - configurable).</li> <li>Implement exponential backoff with jitter to avoid overwhelming the target server during retries.</li> <li>Configure a maximum number of retries per request to prevent infinite loops.</li> </ul> </li> </ul>"},{"location":"operations/error_handling/#worker-level-task-retries-celery","title":"Worker-Level Task Retries (Celery)","text":"<ul> <li>Mechanism: Celery provides built-in support for retrying tasks if they fail.</li> <li>Strategy:<ul> <li>Wrap the core scraping logic within the Celery task in <code>try...except</code> blocks.</li> <li>Catch exceptions that represent potentially recoverable task failures (e.g., persistent fetch errors after fetcher-level retries, temporary infrastructure issues).</li> <li>Use <code>task.retry(exc=exception, countdown=...)</code> to schedule the task for a retry after a specified delay, often with exponential backoff.</li> <li>Configure a maximum number of retries for each Celery task.</li> </ul> </li> </ul>"},{"location":"operations/error_handling/#handling-specific-scraping-errors","title":"Handling Specific Scraping Errors","text":""},{"location":"operations/error_handling/#website-blocks-bans-eg-403-forbidden","title":"Website Blocks / Bans (e.g., 403 Forbidden)","text":"<ul> <li>Detection: Identify status codes (403) or page content changes indicating a block.</li> <li>Immediate Action (Middleware/Worker Logic):<ul> <li>Rotate Proxy IP address immediately for the failed request and subsequent requests to that domain.</li> <li>Rotate User-Agent string.</li> <li>Potentially introduce a longer, domain-specific cool-down period before retrying.</li> </ul> </li> <li>Long-Term Action (Monitoring/Dispatcher): If blocks persist for a specific site across multiple jobs, the central monitoring system should detect this pattern. The Dispatcher might then be configured (manually or automatically based on alerts) to temporarily reduce the scraping frequency or pause job dispatch for that site.</li> </ul>"},{"location":"operations/error_handling/#captchas","title":"CAPTCHAs","text":"<ul> <li>Detection: Identify CAPTCHA presence in page content or via specific selectors/status codes.</li> <li>Strategy:<ul> <li>Avoidance: The primary strategy through polite scraping (delays, AutoThrottle principles), good user agents, and high-quality proxies.</li> <li>Rotation: Try rotating proxies/user agents as CAPTCHAs can be session/IP specific.</li> <li>Solving Services (If unavoidable): Integrate with third-party CAPTCHA solving services (e.g., 2Captcha, Anti-CAPTCHA). This adds cost and complexity and is often a last resort. Requires specific logic within the worker or potentially using commercial scraping APIs that handle this.</li> <li>Failure &amp; Reporting: If CAPTCHAs cannot be bypassed/solved, the job should fail gracefully and log the issue clearly for investigation.</li> </ul> </li> </ul>"},{"location":"operations/error_handling/#dead-letter-queues-dlq","title":"Dead-Letter Queues (DLQ)","text":"<ul> <li>Concept: A separate queue where messages are sent if they fail processing repeatedly (exceeding all retry limits) in the main worker queue.</li> <li>Purpose:<ul> <li>Prevents failing messages from blocking the main queue indefinitely.</li> <li>Allows for investigation of persistently failing jobs without interrupting normal processing.</li> <li>Enables potential manual intervention or bulk reprocessing later.</li> </ul> </li> <li>Implementation: Configure the primary message broker (RabbitMQ, SQS) to route expired or max-retried messages to a designated DLQ. Monitor the DLQ depth via alerting.</li> </ul>"},{"location":"operations/error_handling/#error-reporting-logging","title":"Error Reporting &amp; Logging","text":"<ul> <li>All retry attempts, backoff delays, and terminal failures at both the fetcher and worker levels must be logged comprehensively.</li> <li>Logs should include the target URL, error type/message, stack trace (if applicable), number of attempts, and relevant context (e.g., proxy used).</li> <li>This centralized logging (see Monitoring) is crucial for diagnosing issues, understanding failure patterns, and improving scraper robustness.</li> </ul>"},{"location":"operations/monitoring/","title":"Monitoring &amp; Alerting","text":"<p>Continuous monitoring is essential for understanding system health, identifying performance bottlenecks, diagnosing errors, and ensuring data quality at scale.</p>"},{"location":"operations/monitoring/#log-aggregation","title":"Log Aggregation","text":"<ul> <li>Purpose: Centralize logs from all distributed components (Dispatcher, Workers, Parsers, Django App, Databases) into a single, searchable system.</li> <li>Benefits: Enables comprehensive debugging, tracing requests across services, analyzing error patterns, and understanding application behavior.</li> <li>Tools: ELK Stack (Elasticsearch, Logstash, Kibana), Grafana Loki, Splunk, Datadog Logs, AWS CloudWatch Logs, Google Cloud Logging.</li> <li>Implementation: Configure all services and applications (including Celery and potentially detailed Scrapy stats/logs if applicable) to ship logs (preferably in a structured format like JSON) to the chosen aggregation platform.</li> </ul>"},{"location":"operations/monitoring/#metrics-collection","title":"Metrics Collection","text":"<ul> <li>Purpose: Collect time-series numerical data about system performance and behavior.</li> <li>Benefits: Provides quantitative insights into system health, resource utilization, throughput, and error rates over time. Essential for dashboarding, alerting, and capacity planning.</li> <li>Tools: Prometheus + Grafana, InfluxDB, Datadog, StatsD, AWS CloudWatch Metrics, Google Cloud Monitoring.</li> <li>Key Metrics to Track:<ul> <li>Message Queue:<ul> <li><code>queue_depth</code> (messages ready/unacknowledged)</li> <li><code>message_age</code> (oldest message)</li> <li><code>publish_rate</code> / <code>consume_rate</code></li> </ul> </li> <li>Celery Workers:<ul> <li><code>worker_count</code> (running instances)</li> <li><code>cpu_utilization</code> / <code>memory_utilization</code> (per worker/node)</li> <li><code>task_queued_duration</code> / <code>task_execution_duration</code></li> <li><code>task_success_rate</code> / <code>task_failure_rate</code> / <code>task_retry_rate</code> (per task type)</li> </ul> </li> <li>Scraping Specific (Custom Metrics / Logs):<ul> <li><code>items_scraped_count</code> (per site/total)</li> <li><code>pages_fetched_count</code> (per site/status code)</li> <li><code>error_rate_per_domain</code></li> <li><code>proxy_success_rate</code> / <code>proxy_failure_rate</code></li> <li><code>captcha_detected_count</code></li> </ul> </li> <li>Databases (Config &amp; Structured):<ul> <li><code>connection_count</code></li> <li><code>query_latency</code></li> <li><code>cpu/memory/disk_utilization</code></li> <li><code>replication_lag</code> (if applicable)</li> </ul> </li> <li>Business Metrics:<ul> <li><code>new_jobs_added_per_hour/day</code></li> <li><code>data_freshness</code> (e.g., max time since last update for active sources)</li> </ul> </li> </ul> </li> </ul>"},{"location":"operations/monitoring/#visualization-dashboarding","title":"Visualization &amp; Dashboarding","text":"<ul> <li>Purpose: Create visual representations of collected metrics and log data to provide an at-a-glance overview of system health and performance trends.</li> <li>Tools: Grafana, Kibana, Datadog Dashboards, Google Cloud Monitoring Dashboards, AWS CloudWatch Dashboards.</li> <li>Implementation: Build dashboards displaying key metrics (queue depths, worker status, error rates, scrape rates per site, DB performance). Allow filtering by time range, service, site, etc.</li> </ul>"},{"location":"operations/monitoring/#alerting","title":"Alerting","text":"<ul> <li>Purpose: Proactively notify operators about critical issues or potential problems based on predefined thresholds or patterns in metrics and logs.</li> <li>Tools: Alertmanager (with Prometheus), Grafana Alerting, Datadog Monitors, Cloud provider native alerting (CloudWatch Alarms, Google Cloud Alerting). Integrations with PagerDuty, Slack, etc.</li> <li>Example Alert Conditions:<ul> <li>High message queue depth (above threshold for X minutes).</li> <li>High Celery task failure rate (above Y% over Z minutes).</li> <li>Persistently high error rate for a specific target website.</li> <li>Dead-letter queue depth increasing.</li> <li>Low scrape success rate for critical sources.</li> <li>High CPU/Memory utilization on workers or database nodes.</li> <li>Critical errors detected in aggregated logs.</li> </ul> </li> </ul>"},{"location":"operations/orchestration/","title":"Deployment &amp; Orchestration","text":"<p>Managing the deployment, scaling, and lifecycle of the various system components requires robust orchestration, especially at scale.</p>"},{"location":"operations/orchestration/#containerization","title":"Containerization","text":"<ul> <li>Technology: Docker</li> <li>Purpose: Package each service component (Django app, Dispatcher, Celery workers with scraping dependencies, Parsing service) along with its dependencies into standardized, portable container images.</li> <li>Benefits: Ensures consistency across development, testing, and production environments. Simplifies dependency management. Enables easier scaling and deployment.</li> </ul>"},{"location":"operations/orchestration/#container-orchestration","title":"Container Orchestration","text":"<ul> <li>Technology: Kubernetes (K8s) - potentially using managed services like AWS EKS, Google GKE, or Azure AKS.</li> <li>Purpose: Automate the deployment, scaling, management, and networking of containerized applications.</li> <li>Key Benefits for this System:<ul> <li>Automated Scaling: Use Horizontal Pod Autoscaler (HPA) to automatically scale the number of Celery worker pods based on metrics like message queue depth (requires custom metrics adapter like KEDA) or CPU/memory usage.</li> <li>Deployment Strategies: Perform rolling updates or canary deployments to release new code versions with minimal downtime.</li> <li>Self-Healing: Automatically restarts containers/pods that fail health checks.</li> <li>Resource Management: Define CPU and memory requests/limits for containers to ensure efficient resource allocation and prevent noisy neighbor problems.</li> <li>Service Discovery &amp; Load Balancing: Manages internal communication between services (e.g., workers connecting to databases or queues).</li> <li>Configuration &amp; Secrets Management: Securely manage database credentials, API keys, and other sensitive configuration.</li> </ul> </li> </ul>"},{"location":"operations/orchestration/#infrastructure-as-code-iac","title":"Infrastructure as Code (IaC)","text":"<ul> <li>Technologies: Terraform, Pulumi, AWS CloudFormation, Azure Resource Manager, Google Cloud Deployment Manager.</li> <li>Purpose: Define and manage all cloud infrastructure resources (Kubernetes cluster, managed databases, message queues, S3 buckets, IAM roles, monitoring setup) using declarative configuration files stored in version control.</li> <li>Benefits:<ul> <li>Reproducibility: Easily create identical environments (dev, staging, prod).</li> <li>Automation: Automate infrastructure provisioning and updates.</li> <li>Version Control: Track changes to infrastructure over time.</li> <li>Disaster Recovery: Faster recreation of infrastructure if needed.</li> </ul> </li> </ul>"},{"location":"operations/orchestration/#cicd-pipelines","title":"CI/CD Pipelines","text":"<pre><code>graph TD\n    A[Push Code to Git] --&gt; B(CI Server Triggered);\n    B --&gt; C(Run Tests);\n    C -- Pass --&gt; D(Build Docker Image);\n    D --&gt; E(Push Image to Registry);\n    E --&gt; F(CD Server Triggered);\n    F --&gt; G(Deploy to Kubernetes);\n\n    C -- Fail --&gt; H(Notify Developer);</code></pre> <ul> <li>Technologies: GitHub Actions, GitLab CI, Jenkins, CircleCI.</li> <li>Purpose: Automate the process of building container images, running tests (unit, integration), and deploying updated application code and infrastructure changes.</li> <li>Typical Workflow:<ol> <li>Code pushed to Git repository.</li> <li>CI pipeline triggers: runs tests, performs static analysis.</li> <li>If tests pass, build new Docker images.</li> <li>Push images to a container registry (e.g., Docker Hub, ECR, GCR, ACR).</li> <li>CD pipeline triggers: applies Kubernetes deployment updates, potentially runs IaC tool for infrastructure changes.</li> </ol> </li> </ul>"},{"location":"operations/orchestration/#role-of-managed-cloud-services","title":"Role of Managed Cloud Services","text":"<ul> <li>Purpose: Leverage cloud provider services to reduce operational overhead for common infrastructure components.</li> <li>Examples:<ul> <li>Message Queues: AWS SQS, Google Pub/Sub, Azure Service Bus (handle scaling, availability, durability).</li> <li>Databases: AWS RDS/Aurora, Google Cloud SQL, Azure SQL DB (handle patching, backups, scaling, high availability).</li> <li>Object Storage: AWS S3, Google Cloud Storage, Azure Blob Storage (highly scalable and durable storage).</li> <li>Container Registry: AWS ECR, Google GCR, Azure ACR (store Docker images).</li> <li>Kubernetes: AWS EKS, Google GKE, Azure AKS (manage the K8s control plane).</li> </ul> </li> </ul> <p>By combining these orchestration tools and practices, the system can be deployed, scaled, and managed effectively and reliably.</p>"},{"location":"processing/analysis_storage/","title":"Where to Store Data for Analysis","text":"<p>The primary PostgreSQL database holds the cleaned, structured job data. While excellent for many tasks, different storage systems might be better suited for very large-scale analysis or specific types of processing.</p>"},{"location":"processing/analysis_storage/#postgresql-the-primary-clean-data-store","title":"PostgreSQL: The Primary Clean Data Store","text":"<ul> <li>Role: Main repository for reliable, up-to-date, structured job information.</li> <li>Good For:<ul> <li>Serving data to applications or dashboards.</li> <li>Running standard SQL queries for reports.</li> <li>Providing data for smaller-scale analysis or ML model building (by querying and loading data into tools like Python/Pandas).</li> </ul> </li> <li>Potential Limits: May become slower for extremely complex analytical queries over huge amounts of historical data, or if heavy analysis interferes with ongoing data collection.</li> </ul>"},{"location":"processing/analysis_storage/#considering-other-options-for-scale","title":"Considering Other Options for Scale","text":"<p>As data volume grows or analysis needs become more intensive, these specialized systems might be added alongside PostgreSQL:</p> <pre><code>graph TD\n    A(PostgreSQL Clean Data) --&gt;|Option 1: For Fast Reporting| B(Data Warehouse);\n    A --&gt;|Option 2: For Bulk Processing / ML| C(Data Lake);\n\n    B --&gt; D{BI Tools / SQL Analytics};\n    C --&gt; E{ML Training / Big Data Processing};\n\n    style A fill:#9cf,stroke:#333,stroke-width:2px\n    style B fill:#f9d,stroke:#333,stroke-width:2px\n    style C fill:#ccf,stroke:#333,stroke-width:2px</code></pre>"},{"location":"processing/analysis_storage/#alternativecomplementary-storage-solutions","title":"Alternative/Complementary Storage Solutions","text":""},{"location":"processing/analysis_storage/#1-data-warehouse-eg-snowflake-bigquery-redshift","title":"1. Data Warehouse (e.g., Snowflake, BigQuery, Redshift)","text":"<ul> <li>Purpose: Optimized for Online Analytical Processing (OLAP) - running complex SQL queries over large volumes of data.</li> <li>Architecture: Typically uses columnar storage, massively parallel processing (MPP).</li> <li>Use Case:<ul> <li>Business Intelligence (BI) and reporting using tools like Tableau, Power BI, Looker.</li> <li>Running complex SQL aggregations, window functions over historical job data.</li> <li>Providing a performant, dedicated resource for analytical queries, separate from the operational database.</li> </ul> </li> <li>Integration: Requires an ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform) process to periodically copy and potentially reshape data from the operational PostgreSQL database into the data warehouse. Tools like Airflow, dbt, Fivetran, or custom scripts can manage this.</li> </ul>"},{"location":"processing/analysis_storage/#2-data-lake-eg-s3-gcs-adls-query-engines","title":"2. Data Lake (e.g., S3, GCS, ADLS + Query Engines)","text":"<ul> <li>Purpose: Store vast amounts of structured, semi-structured, and unstructured data cost-effectively in open file formats. Decouple storage from compute.</li> <li>Architecture: Stores data as files (e.g., Apache Parquet, ORC) on object storage. Uses query engines (e.g., Apache Spark, Presto/Trino, AWS Athena) or processing frameworks to interact with the data.</li> <li>Use Case:<ul> <li>Storing large datasets for ML model training (Spark MLlib, PyTorch/TensorFlow often read efficiently from Parquet).</li> <li>Exploratory data analysis by data scientists using tools like Spark or Dask.</li> <li>Archiving historical processed data cost-effectively.</li> <li>Handling data volumes potentially exceeding comfortable/cost-effective limits for a data warehouse.</li> </ul> </li> <li>Integration: Requires a process (often using Spark, Dask, or Airflow) to extract data from PostgreSQL and write it to the data lake in an optimized format (like partitioned Parquet files).</li> </ul>"},{"location":"processing/analysis_storage/#recommendation-path","title":"Recommendation Path","text":"<ol> <li>Start with PostgreSQL: Use the primary PostgreSQL database (containing cleaned, structured data) as the initial source for both analysis and ML data extraction. Optimize it with appropriate indexing.</li> <li>Monitor Performance: Observe query performance and database load as data volume grows and analytical/ML workloads increase.</li> <li>Introduce Data Warehouse (If Needed for BI/Analytics): If complex SQL-based analytics and BI reporting become slow or interfere with operational performance, implement an ELT process to move data into a dedicated Data Warehouse (Snowflake, BigQuery, Redshift).</li> <li>Introduce Data Lake (If Needed for Large-Scale ML/Storage): If ML training datasets become very large, or if cost-effective storage for processed historical data is required, implement a pipeline to export data (e.g., daily/hourly snapshots) from PostgreSQL to a Data Lake (S3/GCS/ADLS as Parquet files). ML training pipelines can then read directly from the Data Lake.</li> </ol> <p>PostgreSQL serves as an excellent, reliable source, but complementing it with a Data Warehouse or Data Lake provides optimized solutions for specific large-scale analytical and ML workloads.</p>"},{"location":"processing/ml_prep/","title":"Preparing Data for Computer Analysis (ML)","text":"<p>If the goal is to use the collected job data for tasks like predicting salaries, classifying job types automatically, or building recommendation systems (often called Machine Learning or ML), the data needs further transformation into a purely numerical format that computer algorithms can understand.</p>"},{"location":"processing/ml_prep/#where-this-happens","title":"Where This Happens","text":"<ul> <li>This preparation usually happens separately from the main database cleaning, often right before the analysis or prediction task begins.</li> <li>It's typically done using programming tools (like Python with specific data analysis libraries) after loading the required clean data from PostgreSQL.</li> </ul>"},{"location":"processing/ml_prep/#general-goal-data-to-numbers","title":"General Goal: Data to Numbers","text":"<p>Machine learning algorithms work primarily with numbers. Therefore, the main goal is to convert all relevant pieces of information, especially text and categories, into a numerical representation.</p> <pre><code>graph TD\n    subgraph Clean Data from DB\n        A(Job Title);\n        B(Employment Type Category);\n        C(Salary Range Numbers);\n        D(Description Text);\n    end\n\n    subgraph ML Preparation Steps\n        direction LR\n        P1(Choose Relevant Info);\n        P2(Convert Text to Numbers);\n        P3(Convert Categories to Numbers);\n        P4(Adjust Numerical Ranges);\n    end\n\n    subgraph Ready for ML Algorithm\n        Z[Numerical Features];\n    end\n\n    A --&gt; P1;\n    B --&gt; P1;\n    C --&gt; P1;\n    D --&gt; P1;\n\n    P1 -- Selected Text --&gt; P2;\n    P1 -- Selected Categories --&gt; P3;\n    P1 -- Selected Numbers --&gt; P4;\n\n    P2 --&gt; Z;\n    P3 --&gt; Z;\n    P4 --&gt; Z;</code></pre>"},{"location":"processing/ml_prep/#common-ml-preprocessing-steps","title":"Common ML Preprocessing Steps","text":""},{"location":"processing/ml_prep/#1-feature-selection","title":"1. Feature Selection","text":"<p>Goal: Select only the pieces of data (features) that are actually useful for the specific prediction or analysis task.</p> <p>Action: Decide which columns from the cleaned data table to use. For example, to predict salary, you might use job title, location, company size, and required skills, but maybe not the raw job ID.</p>"},{"location":"processing/ml_prep/#2-text-processing-for-description_text-job_title_raw","title":"2. Text Processing (for <code>description_text</code>, <code>job_title_raw</code>)","text":"<p>Goal: Represent words and sentences numerically so algorithms can find patterns.</p> <p>Action: This involves techniques that might:</p> <p>Count how often important words appear.</p> <p>Assign scores to words based on their relevance.</p> <p>Create numerical codes (vectors or embeddings) that capture the meaning or context of words or entire descriptions.</p> <p>This is often applied to fields like job_title and description_text.</p>"},{"location":"processing/ml_prep/#3-categorical-feature-encoding","title":"3. Categorical Feature Encoding","text":"<p>Goal: Represent non-numerical labels (like 'Full-time', 'Part-time', or specific city names if used as categories) numerically.</p> <p>Action: Assign numerical codes to each category. This could be:</p> <p>Simple unique numbers (e.g., Full-time=1, Part-time=2).</p> <p>Creating separate \"flag\" columns (0 or 1) for each possible category.</p>"},{"location":"processing/ml_prep/#4-numerical-feature-scaling","title":"4. Numerical Feature Scaling","text":"<p>Goal: Ensure that numerical fields with very different ranges (e.g., salary in thousands vs. years of experience from 1-10) don't unfairly influence certain algorithms.</p> <p>Action: Rescale numbers to a common range (like 0 to 1, or centered around zero). This helps many algorithms learn more effectively.</p>"},{"location":"processing/ml_prep/#5-splitting-data","title":"5. Splitting Data","text":"<p>Goal: Check if the analysis or prediction model works on data it hasn't seen before.</p> <p>Action: Before using the data, it's usually split into a 'training' set (to build the model) and a 'testing' set (to evaluate how well it performs).</p> <p>These steps transform the clean, structured data into a numerical format suitable for ingestion by machine learning algorithms. The specific steps and methods chosen depend heavily on the ML task and the selected algorithm.</p>"},{"location":"processing/workflow/","title":"Data Preprocessing Workflow","text":"<p>Once raw data is extracted and stored in the structured database (PostgreSQL), a dedicated preprocessing workflow is required to clean, standardize, and enrich the data, making it suitable for analysis or machine learning applications. This process typically runs after the initial parsing and insertion.</p>"},{"location":"processing/workflow/#workflow-overview","title":"Workflow Overview","text":"<p>The following diagram illustrates the key steps in the preprocessing pipeline:</p> <pre><code>graph LR\n    A[Structured Data in PostgreSQL] --&gt; B(Select Data for Processing);\n    B --&gt; C(Check for &amp; Handle Duplicates);\n    C --&gt; D(Fill Obvious Missing Information);\n    D --&gt; E(Clean &amp; Format Text Fields);\n    E --&gt; F(Structure Specific Fields - Location/Salary);\n\n    style A fill:#9cf,stroke:#333,stroke-width:2px\n    style F fill:#9cf,stroke:#333,stroke-width:2px</code></pre>"},{"location":"processing/workflow/#detailed-steps","title":"Detailed Steps","text":""},{"location":"processing/workflow/#1-load-data","title":"1. Load Data","text":"<p>Goal: Choose which records need cleaning (e.g., newly added jobs).</p> <p>Action: Query the database for relevant records.</p>"},{"location":"processing/workflow/#2-handle-duplicates","title":"2. Handle Duplicates","text":"<p>Goal: Avoid having the exact same job listed multiple times unnecessarily.</p> <p>Action: Find records that seem identical (based on URL, job ID, company/title). Decide whether to merge them, keep the best one, or just note that they are duplicates.</p>"},{"location":"processing/workflow/#3-handle-missing-values","title":"3. Handle Missing Values","text":"<p>Goal: Deal with empty fields where possible.</p> <p>Action: For some fields, it might make sense to fill missing values (e.g., mark a missing employment_type as 'Unknown'). For others (like salary), leaving them empty might be better than guessing. Sometimes, records with too much missing critical info might be flagged or removed.</p>"},{"location":"processing/workflow/#4-clean-standardize-text","title":"4. Clean &amp; Standardize Text","text":"<p>Goal: Make text consistent and easy to read/process.</p> <p>Action: Remove extra spaces or leftover HTML code. Ensure consistent capitalization (e.g., make everything lowercase for easier searching, or proper case for display). Fix any text encoding problems.</p> <pre><code>graph LR\n    F(Structure Specific Fields - Location/Salary) --&gt; G(Review for Unusual Values - Optional);\n    G --&gt; H(Add Extra Info - Optional Enrichment);\n    H --&gt; I(Final Quality Check);\n    I --&gt; J(Save Cleaned Data);\n\n    style F fill:#9cf,stroke:#333,stroke-width:2px\n    style J fill:#9cf,stroke:#333,stroke-width:2px</code></pre>"},{"location":"processing/workflow/#5-parse-structure-fields","title":"5. Parse &amp; Structure Fields","text":"<p>Goal: Extract detailed information hidden within text fields.</p> <p>Action:</p> <p>Location: Try to identify city, country, and remote status from the raw location text (e.g., \"London, UK\" -&gt; City: London, Country: UK).</p> <p>Salary: Try to find minimum/maximum amounts, currency (like $, \u00a3, \u20ac), and pay period (like 'per year', 'per hour') from the salary text.</p> <p>Store these structured pieces (perhaps using flexible JSONB fields in PostgreSQL).</p>"},{"location":"processing/workflow/#6-outlier-detectionhandling-optional","title":"6. Outlier Detection/Handling (Optional)","text":"<p>Goal: Catch potential errors that look like extreme numbers.</p> <p>Action: If numbers were extracted (like salary ranges), check if any look impossibly high or low. These might be flagged for review.</p>"},{"location":"processing/workflow/#7-data-enrichment-optional","title":"7. Data Enrichment (Optional)","text":"<p>Goal: Enhance the data with information not directly in the job post.</p> <p>Action:</p> <p>Maybe add company details (like industry or size) by looking up the company name elsewhere.</p> <p>Try to automatically identify key skills (like \"Python\", \"Java\") mentioned in the job description.</p> <p>Attempt to guess a standard job category (like 'Software Engineer') or seniority level ('Junior', 'Senior') based on the title and description</p>"},{"location":"processing/workflow/#8-validate-data-quality","title":"8. Validate Data Quality","text":"<p>Goal: Ensure the processed data looks reasonable.</p> <p>Action: Perform automated checks (e.g., are dates valid? Are required fields filled?). Flag records that fail checks.</p>"},{"location":"processing/workflow/#9-updateload-processed-data","title":"9. Update/Load Processed Data","text":"<p>Goal: Ensure the processed data looks reasonable.</p> <p>Action: Perform automated checks (e.g., are dates valid? Are required fields filled?). Flag records that fail checks.</p>"},{"location":"scraping/anti_scraping/","title":"Handling Anti-Scraping Measures","text":"<p>Many websites employ techniques to detect and block automated scraping.</p> Technique Problem Addressed Solution Mechanism User-Agent Rotation Default/Repeated UA detection Cycle through realistic browser UAs Proxy Rotation (IP) Source IP blocking/rate limiting Route requests via different proxy IPs Realistic Headers Missing/Unusual request headers Include standard browser headers Delays &amp; Jitter Unnatural request timing/rate Add pauses/randomness between actions Browser Automation JS Execution/Rendering checks Use Playwright/Selenium Fingerprint Masking Advanced JS-based detection Use stealth plugins/specialized tools"},{"location":"scraping/anti_scraping/#core-strategies","title":"Core Strategies","text":""},{"location":"scraping/anti_scraping/#1-rotating-user-agents","title":"1. Rotating User-Agents","text":"<ul> <li>Problem: Sending requests with a default library User-Agent (like <code>python-requests</code>) is an easy giveaway. Repeated requests with the same User-Agent can also be flagged.</li> <li>Solution:<ul> <li>Maintain a list of realistic, common browser User-Agent strings.</li> <li>Implement logic (e.g., middleware if using Scrapy, or direct logic in the worker's fetching function) to randomly select a User-Agent from the list for each request or session.</li> <li>Ensure the list is periodically updated with current browser versions.</li> <li>Libraries like <code>fake-useragent</code> can help generate realistic UAs.</li> </ul> </li> </ul>"},{"location":"scraping/anti_scraping/#2-using-proxies-ip-rotation","title":"2. Using Proxies (IP Rotation)","text":"<ul> <li>Problem: Making many requests from the same IP address is the most common reason for getting blocked or rate-limited.</li> <li>Solution: Route outgoing requests through proxy servers.<ul> <li>Proxy Pool: Maintain a pool of proxy IP addresses (sourced from commercial proxy providers or internal infrastructure). Providers often specialize (datacenter vs. residential IPs).</li> <li>Rotation Logic: Implement logic (middleware or within the worker) to select a different proxy IP for each request or after a certain number of requests/failures from a single IP.</li> <li>Proxy Types: Datacenter proxies are cheaper but easier to detect; Residential proxies are more expensive but harder to distinguish from real users. Choice depends on target site sensitivity and budget.</li> <li>Management: Handle proxy authentication, check proxy health (disable failing ones), manage session persistence if needed (sticky IPs). Commercial proxy services often handle much of this via API gateways.</li> </ul> </li> <li>Implementation: Configure the <code>requests</code> session or Playwright browser launch options to use the selected proxy for each request.</li> </ul>"},{"location":"scraping/anti_scraping/#3-realistic-request-headers","title":"3. Realistic Request Headers","text":"<ul> <li>Problem: Missing or unusual HTTP headers can signal a bot.</li> <li>Solution: Include standard browser headers like <code>Accept</code>, <code>Accept-Language</code>, <code>Accept-Encoding</code>, and sometimes <code>Referer</code> (set appropriately based on navigation flow). Ensure header order appears natural if possible, though often less critical than User-Agent and IP.</li> </ul>"},{"location":"scraping/anti_scraping/#4-mimicking-human-behavior-delays-timing","title":"4. Mimicking Human Behavior (Delays &amp; Timing)","text":"<ul> <li>Problem: Rapid-fire, perfectly timed requests are unnatural.</li> <li>Solution:<ul> <li>Implement politeness delays between requests (as covered in Rate Limiting).</li> <li>Introduce slight randomization (jitter) into delays.</li> <li>If using browser automation (Playwright), add small, randomized delays between actions (clicks, scrolls) where appropriate.</li> </ul> </li> </ul>"},{"location":"scraping/anti_scraping/#5-handling-javascript-challenges","title":"5. Handling JavaScript Challenges","text":"<ul> <li>Problem: Some sites use JavaScript execution, browser fingerprinting (checking fonts, screen resolution, plugins), or canvas fingerprinting to detect bots.</li> <li>Solution:<ul> <li>Using real browser automation (Playwright) inherently solves many basic JS execution challenges.</li> <li>Advanced fingerprinting may require specialized Playwright configurations (e.g., <code>playwright-stealth</code> adaptations) or using sophisticated commercial proxy/browser services that attempt to mask these attributes. This is complex and often site-specific.</li> </ul> </li> </ul>"},{"location":"scraping/anti_scraping/#gradual-approach","title":"Gradual Approach","text":"<p>Start with basic politeness, User-Agent rotation, and good quality proxies. Introduce more complex techniques only if necessary based on monitoring data showing blocks or failures on specific target sites.</p>"},{"location":"scraping/challenges/","title":"Handling Scraping Challenges Overview","text":"<p>Web scraping frequently encounters obstacles designed to prevent automated access or arising from website design. This section outlines strategies for common challenges.</p> <p>Sub-sections cover specific tactics for:</p> <ul> <li>Rate Limiting &amp; Politeness: Avoiding overwhelming servers and handling \"Too Many Requests\" errors.</li> <li>Pagination &amp; Infinite Scroll: Navigating through multiple pages or dynamically loaded lists of results.</li> <li>Anti-Scraping Measures: Dealing with mechanisms explicitly designed to detect and block scrapers (proxies, user-agents).</li> </ul>"},{"location":"scraping/fetching_dynamic_content/","title":"Handling Dynamic Content (Playwright Decision)","text":"<p>Many modern job boards rely heavily on JavaScript to load content dynamically, render UI components, handle user interactions, and implement infinite scrolling.</p> <pre><code>graph TD\n    A[Start Playwright Task] --&gt; B(Launch Browser);\n    B --&gt; C(Navigate to URL);\n    C --&gt; D{Wait for Content/Selector};\n    D --&gt; E(Perform Interactions?);\n    E -- Yes --&gt; F(Click/Scroll/etc.);\n    F --&gt; D;\n    E -- No --&gt; G(Extract Page Source/Data);\n    G --&gt; H(Close Browser);\n    H --&gt; I[Task Complete];</code></pre>"},{"location":"scraping/fetching_dynamic_content/#challenges-with-dynamic-content","title":"Challenges with Dynamic Content","text":"<ul> <li>Data Not in Initial HTML: Job listings, pagination controls, or filter results might not be present in the initial HTML source returned by a simple HTTP request.</li> <li>Client-Side Rendering: Frameworks like React, Vue, Angular render content in the user's browser using JavaScript.</li> <li>User Interaction Required: Content might only load after clicking buttons (\"Load More\"), scrolling down (\"Infinite Scroll\"), or interacting with forms.</li> </ul>"},{"location":"scraping/fetching_dynamic_content/#technology-choice-playwright","title":"Technology Choice: Playwright","text":"<ul> <li>Rationale: When dynamic content loading or complex user interaction is necessary, a browser automation tool is required. Playwright is chosen for its modern API, good performance relative to other automation tools, and robust feature set (network interception, multiple browser support).</li> <li>Integration: Playwright will be executed within the Celery workers for specific tasks identified as needing browser automation. The <code>scrapy-playwright</code> library could be leveraged if Scrapy were the primary driver, but in a direct Celery task, the <code>playwright</code> library would be used directly.</li> </ul>"},{"location":"scraping/fetching_dynamic_content/#implementation-strategy","title":"Implementation Strategy","text":"<ul> <li>Selective Use: Playwright will only be used for websites or specific scraping stages explicitly configured as requiring it due to the high resource cost (CPU/RAM).</li> <li>Task Logic: Celery tasks designated for Playwright usage will:<ol> <li>Launch a browser instance (e.g., Chromium).</li> <li>Navigate to the target URL.</li> <li>Wait for specific elements or network conditions indicating content has loaded (using Playwright's built-in waiting mechanisms).</li> <li>Perform necessary interactions (clicking, scrolling).</li> <li>Extract the fully rendered page source (<code>page.content()</code>) or specific data points directly using Playwright's locators.</li> <li>Close the browser context/page gracefully to free up resources.</li> </ol> </li> <li>Resource Management: Worker concurrency and server resources must be carefully managed when running Playwright tasks. Kubernetes resource requests/limits and autoscaling based on load are crucial.</li> </ul>"},{"location":"scraping/fetching_dynamic_content/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Selenium: Similar capabilities but often considered slightly less modern API-wise. Suffers from the same resource intensity.</li> <li>Splash: A headless browser rendering service. Shifts resource load to a central Splash instance but adds operational complexity (managing Splash).</li> <li>Commercial Browser APIs: Services like ScrapingBee, Browserless.io offer browser rendering as a service. Offloads resource management entirely but incurs direct costs per request/subscription.</li> <li>API Reverse Engineering: Always the preferred alternative. If the underlying data can be accessed via an API call discovered through network analysis, Playwright should be avoided.</li> </ul> <p>Playwright provides the necessary power for dynamic sites but will be employed judiciously due to its performance implications.</p>"},{"location":"scraping/fetching_lightweight/","title":"Core Fetching Implementation (Requests)","text":"<p>For websites or specific endpoints that serve content as static HTML or via predictable APIs, a lightweight fetching approach is preferred for efficiency.</p>"},{"location":"scraping/fetching_lightweight/#technology-choice-requests-library","title":"Technology Choice: <code>requests</code> Library","text":"<ul> <li>Rationale: <code>requests</code> is the de facto standard, robust, and well-maintained Python library for making HTTP requests. It's significantly lighter in terms of resource consumption (CPU/RAM) compared to full browser automation.</li> </ul>"},{"location":"scraping/fetching_lightweight/#key-implementation-features","title":"Key Implementation Features","text":"<pre><code>graph TD\n    A[Start Fetch Request] --&gt; B{Make HTTP Request};\n    B --&gt; C{Check Response Status};\n    C -- OK (2xx) --&gt; D[Success: Return Response];\n    C -- Retryable Error (e.g., 503, Timeout)? --&gt; E{Retry Count &lt; Max?};\n    E -- Yes --&gt; F[Apply Backoff Delay];\n    F --&gt; B;\n    E -- No --&gt; G[Fail: Raise Exception];\n    C -- Non-Retryable Error (e.g., 404) --&gt; G;\n\n    style D fill:#cfc,stroke:#333,stroke-width:2px\n    style G fill:#fcc,stroke:#333,stroke-width:2px</code></pre>"},{"location":"scraping/fetching_lightweight/#retry-logic","title":"Retry Logic","text":"<ul> <li>Mechanism: Utilize the <code>urllib3.Retry</code> mechanism integrated with <code>requests.Session</code> or the <code>backoff</code> decorator library.</li> <li>Configuration: Configure automatic retries for:<ul> <li>Transient network errors (timeouts, connection issues).</li> <li>Specific server-side error codes (e.g., 500, 502, 503, 504).</li> <li>Potentially rate-limiting codes (e.g., 429) if deemed temporary.</li> </ul> </li> <li>Backoff Strategy: Employ exponential backoff with jitter between retries to avoid overwhelming servers.</li> </ul>"},{"location":"scraping/fetching_lightweight/#session-management","title":"Session Management","text":"<ul> <li>Use <code>requests.Session</code> objects for:<ul> <li>Connection Pooling: Reusing underlying TCP connections for better performance when making multiple requests to the same host.</li> <li>Cookie Persistence: Automatically handle cookies if needed for session management on the target site (though less common for API endpoints).</li> </ul> </li> </ul>"},{"location":"scraping/fetching_lightweight/#headers-user-agent","title":"Headers &amp; User-Agent","text":"<ul> <li>User-Agent Rotation: Set appropriate <code>User-Agent</code> headers, ideally rotating them via middleware or logic within the worker to mimic different browsers and reduce blocking potential.</li> <li>Other Headers: Include other standard headers (<code>Accept</code>, <code>Accept-Language</code>, etc.) to appear more like a regular browser request.</li> </ul>"},{"location":"scraping/fetching_lightweight/#timeout-configuration","title":"Timeout Configuration","text":"<ul> <li>Set reasonable timeouts for connection and read operations to prevent tasks from hanging indefinitely on unresponsive servers.</li> </ul>"},{"location":"scraping/fetching_lightweight/#when-to-use","title":"When to Use","text":"<ul> <li>Fetching <code>robots.txt</code>.</li> <li>Accessing static HTML pages where job data is directly embedded.</li> <li>Interacting with identified APIs (AJAX/XHR/Fetch) that return data (often JSON).</li> <li>Simple pagination scenarios handled by URL parameters.</li> </ul> <p>By defaulting to this lightweight approach and only escalating to browser automation when necessary, the system optimizes resource usage and improves overall throughput.</p>"},{"location":"scraping/pagination/","title":"Handling Pagination &amp; Infinite Scroll","text":"<p>Job listings are often spread across multiple pages or loaded dynamically.</p>"},{"location":"scraping/pagination/#types-of-pagination-strategies","title":"Types of Pagination &amp; Strategies","text":""},{"location":"scraping/pagination/#1-next-page-links-page-number-links","title":"1. Next Page Links / Page Number Links","text":"<ul> <li>Mechanism: Standard HTML links (<code>&lt;a&gt;</code> tags) point to the next page or specific page numbers. URLs often change predictably (e.g., <code>?page=2</code>, <code>?p=3</code>).</li> <li>Strategy:<ol> <li>Parse the current page to find the link to the next page (using CSS selectors or XPath).</li> <li>If found, generate a request for the next page's URL.</li> <li>Repeat until no \"next page\" link is found or a predefined page limit is reached.</li> <li>This can be handled within a single worker task, looping through pages sequentially or yielding requests if using a Scrapy-like internal flow.</li> </ol> </li> </ul>"},{"location":"scraping/pagination/#2-javascript-driven-pagination-load-more-buttons","title":"2. JavaScript-Driven Pagination (\"Load More\" Buttons)","text":"<ul> <li>Mechanism: Clicking a button executes JavaScript that fetches and inserts more results onto the current page, or triggers a background API call.</li> <li>Strategy:<ul> <li>Option A (Browser Automation - Playwright): Use Playwright to simulate clicking the \"Load More\" button repeatedly, waiting for new content to appear after each click, until the button disappears or no new content loads. Extract data from the fully loaded page.</li> <li>Option B (API Reverse Engineering): Monitor network traffic when clicking the button. Identify the underlying API request. If found, call this API directly (likely passing page numbers or cursors as parameters) using the lightweight <code>requests</code> library. This is strongly preferred if possible.</li> </ul> </li> </ul>"},{"location":"scraping/pagination/#3-infinite-scroll","title":"3. Infinite Scroll","text":"<ul> <li>Mechanism: New content automatically loads as the user scrolls down the page, often triggering background API calls.</li> <li>Strategy:<ul> <li>Option A (Browser Automation - Playwright): Use Playwright to repeatedly scroll down the page (<code>page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")</code>), waiting for new content elements or network activity to cease after each scroll, until no new content appears. Extract data from the fully loaded page.</li> <li>Option B (API Reverse Engineering): Monitor network traffic during scrolling. Identify the API calls fetching batches of results. Replicate these API calls directly using <code>requests</code>, passing appropriate parameters (page number, cursor, offset). This is strongly preferred if possible.</li> </ul> </li> </ul>"},{"location":"scraping/pagination/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>The specific pagination method needs to be identified during initial website analysis and stored in the site's configuration.</li> <li>The worker task logic will branch based on the configured pagination type for the target site.</li> <li>Care must be taken to detect the end condition (no more pages/content) to avoid infinite loops.</li> </ul>"},{"location":"scraping/rate_limiting/","title":"Handling Rate Limiting &amp; Politeness","text":"<p>Respecting website resources and avoiding rate limits is crucial for ethical and sustainable scraping.</p> <pre><code>graph TD\n    A[Receive HTTP Response] --&gt; B{Check Status Code};\n    B -- OK (2xx) --&gt; C[Process Response];\n    B -- Rate Limit (429)? --&gt; D{Retry Logic};\n    D -- Try Again After Backoff --&gt; E[Re-queue Request/Retry];\n    D -- Max Retries Reached --&gt; F[Log Error / Mark Job Failed];\n    B -- Server Error (5xx)? --&gt; D;\n    B -- Other Client Error (4xx)? --&gt; F;\n    B -- Redirect (3xx)? --&gt; G[Handle Redirect];\n\n    style C fill:#cfc,stroke:#333,stroke-width:2px\n    style F fill:#fcc,stroke:#333,stroke-width:2px</code></pre>"},{"location":"scraping/rate_limiting/#politeness-delays","title":"Politeness Delays","text":"<ul> <li>Mechanism: Introduce artificial delays between consecutive requests to the same domain.</li> <li>Implementation:<ul> <li>Use a base <code>DOWNLOAD_DELAY</code> configured per site or globally (e.g., 1-3 seconds).</li> <li>Respect <code>Crawl-delay</code> directive from <code>robots.txt</code> if present and stricter than the base delay.</li> <li>Consider dynamic delays (similar to Scrapy's AutoThrottle) that adjust based on server response times, although this adds complexity outside the Scrapy framework. Logic within the worker could increase delays if rate-limiting responses (429) are encountered.</li> </ul> </li> </ul>"},{"location":"scraping/rate_limiting/#concurrency-limits","title":"Concurrency Limits","text":"<ul> <li>Mechanism: Limit the number of simultaneous requests being made to the same website domain or IP address.</li> <li>Implementation:<ul> <li>If using Celery workers without Scrapy's internal scheduler managing concurrency per domain, this needs careful management.</li> <li>Potential strategies include: using distributed locks (based on domain name) via Redis/ZooKeeper, limiting worker concurrency per domain via routing keys/queues, or implementing domain-aware throttling within the worker's fetching logic.</li> <li>Kubernetes resource limits also implicitly limit overall concurrency.</li> </ul> </li> </ul>"},{"location":"scraping/rate_limiting/#handling-rate-limit-responses-http-429","title":"Handling Rate Limit Responses (HTTP 429)","text":"<ul> <li>Detection: Identify the <code>HTTP 429 Too Many Requests</code> status code (and potentially others used for rate limiting).</li> <li>Strategy:<ul> <li>Treat as a trigger for the fetcher-level retry mechanism (<code>requests</code>+<code>Retry</code>).</li> <li>Ensure the retry logic includes a significant backoff delay (exponential backoff) specifically for 429 errors.</li> <li>Log these events clearly.</li> <li>If 429 errors persist for a site, it's a strong signal that base delays need increasing or concurrency needs reducing for that domain (potentially requiring configuration updates via monitoring feedback).</li> </ul> </li> </ul>"},{"location":"scraping/raw_storage/","title":"Raw Data Storage","text":"<p>Storing the raw, unprocessed HTML content fetched from target websites is a crucial step for resilience and decoupling.</p> <pre><code>graph TD\n    A[Worker Fetches HTML] --&gt; B(Compress Content);\n    B --&gt; C(Construct S3 Path/Filename);\n    C --&gt; D[Upload to S3 Bucket];\n    D --&gt; E(Log S3 Path);</code></pre>"},{"location":"scraping/raw_storage/#storage-choice-cloud-object-storage","title":"Storage Choice: Cloud Object Storage","text":"<ul> <li>Technology: AWS S3, Google Cloud Storage (GCS), Azure Blob Storage.</li> <li>Rationale:<ul> <li>Scalability: Virtually unlimited storage capacity.</li> <li>Durability &amp; Availability: Designed for high durability and availability, reducing risk of data loss.</li> <li>Cost-Effectiveness: Generally cheaper than database storage for large volumes of raw data.</li> <li>Decoupling: Separates the fetching process from the parsing process.</li> <li>Integration: Easily integrates with other cloud services (e.g., triggering parsing functions via S3 events).</li> </ul> </li> </ul>"},{"location":"scraping/raw_storage/#storage-format-structure","title":"Storage Format &amp; Structure","text":""},{"location":"scraping/raw_storage/#content","title":"Content","text":"<ul> <li>Store the full HTML content exactly as received from the target website after fetching (and rendering, if Playwright was used).</li> </ul>"},{"location":"scraping/raw_storage/#file-naming-convention","title":"File Naming Convention","text":"<ul> <li>Use a consistent and informative naming convention that allows easy retrieval and avoids collisions. Example structure:     <code>{source_website}/{job_id_or_hash}/{scrape_timestamp}.html.gz</code><ul> <li><code>source_website</code>: e.g., <code>linkedin</code>, <code>indeed_com</code></li> <li><code>job_id_or_hash</code>: A unique identifier for the job (ideally from the source site, or a hash of the URL/content if no ID exists).</li> <li><code>scrape_timestamp</code>: ISO 8601 format timestamp (e.g., <code>2025-03-27T183000Z</code>).</li> </ul> </li> <li>Consider partitioning within the bucket (using prefixes that look like folders) by date (e.g., <code>year=2025/month=03/day=27/</code>) for efficient querying by date ranges if using tools like AWS Athena later.</li> </ul>"},{"location":"scraping/raw_storage/#compression","title":"Compression","text":"<ul> <li>Method: Compress the HTML content using <code>gzip</code> (<code>.gz</code>) or <code>brotli</code> (<code>.br</code>) before uploading.</li> <li>Benefits: Significantly reduces storage space requirements (HTML compresses well) and associated costs. Also reduces data transfer time/cost.</li> <li>Implementation: Use standard Python libraries (<code>gzip</code>, <code>brotli</code>) in the worker process after fetching the content and before uploading to the object storage bucket. The parsing service will need to decompress the files upon retrieval.</li> </ul>"},{"location":"scraping/raw_storage/#process-integration","title":"Process Integration","text":"<ul> <li>The scraping worker (Celery task) is responsible for fetching the content, optionally compressing it, and uploading it to the designated S3 bucket path.</li> <li>The S3 object key (path) should ideally be included in any subsequent message triggering the parsing stage, or be derivable from the job metadata.</li> </ul>"},{"location":"scraping/scrape_flow/","title":"Scrape Flow: Search vs. Detail Pages","text":"<p>Scraping job listings typically involves a two-stage process: first identifying potential jobs on search result pages, and second, extracting detailed information from individual job posting pages.</p>"},{"location":"scraping/scrape_flow/#stage-1-search-result-page-scraping","title":"Stage 1: Search Result Page Scraping","text":"<ul> <li>Goal: Identify URLs of potentially relevant individual job postings based on configured search criteria (keywords, location, filters).</li> <li>Input: A configured search task (e.g., target URL for a specific search on Site X).</li> <li>Process:<ol> <li>Worker fetches the search results page (using <code>requests</code> or Playwright depending on site complexity).</li> <li>Handles pagination/infinite scroll to navigate through multiple pages of results, if necessary.</li> <li>Parses the search result page(s) to extract:<ul> <li>Job Detail Page URLs: The primary output.</li> <li>(Optional) Basic metadata visible on the search page (e.g., title snippet, company, location, date posted, sometimes salary snippet).</li> <li>(Optional) Unique Job IDs if available.</li> </ul> </li> <li>Stores the raw HTML of the search result page(s) in S3 for auditing/debugging.</li> </ol> </li> <li>Output: A list of Job Detail Page URLs (and potentially basic associated metadata).</li> </ul>"},{"location":"scraping/scrape_flow/#stage-2-job-detail-page-scraping","title":"Stage 2: Job Detail Page Scraping","text":"<ul> <li>Goal: Extract comprehensive, structured information from an individual job posting page.</li> <li>Input: A specific Job Detail Page URL obtained from Stage 1.</li> <li>Process:<ol> <li>Worker fetches the job detail page (often simpler, potentially just <code>requests</code>, but may still need Playwright if content loads dynamically).</li> <li>Parses the page content to extract detailed fields (Job Title, Company Name, Full Description, Location, Salary Details, Employment Type, etc.) based on predefined selectors for that site.</li> <li>Stores the raw HTML of the job detail page in S3.</li> </ol> </li> <li>Output: Raw HTML stored in S3, ready for the dedicated Parsing Service to extract structured data.</li> </ul>"},{"location":"scraping/scrape_flow/#triggering-stage-2-from-stage-1","title":"Triggering Stage 2 from Stage 1","text":"<p>There are two primary models for managing this flow within the event-driven architecture:</p>"},{"location":"scraping/scrape_flow/#model-a-internal-handling-eg-within-scrapy-or-similar-logic","title":"Model A: Internal Handling (e.g., within Scrapy or similar logic)","text":"<ul> <li>Description: A single job message triggers the worker to start the Stage 1 scrape. As the worker parses search results and finds detail page URLs, it internally generates and schedules requests for those Stage 2 pages within the same process/framework execution context (similar to how <code>yield scrapy.Request</code> works).</li> <li>Pros: Simpler from an external orchestration perspective; keeps related scrape activities together; efficient if using a framework like Scrapy that manages internal requests well.</li> <li>Cons: A single long-running task; failure during Stage 2 fetching might complicate overall job status reporting for the initial Stage 1 message.</li> </ul>"},{"location":"scraping/scrape_flow/#model-b-external-dispatching","title":"Model B: External Dispatching","text":"<ul> <li>Description: The worker processing the Stage 1 job message fetches search results. For each Job Detail URL found, it creates and publishes a new, distinct job message onto the central message queue, specifically for that Stage 2 URL. Separate worker instances pick up these Stage 2 messages.</li> <li>Pros: Granular jobs; Stage 2 failures are isolated; potentially better load distribution if Stage 2 tasks are numerous or long-running.</li> <li>Cons: Increased message volume on the central queue; requires careful deduplication logic (don't dispatch Stage 2 job if already processed recently); more complex orchestration.</li> </ul> <p>Chosen Approach:</p> <ul> <li>While Model B offers granularity, Model A (Internal Handling) is often simpler to manage initially, especially if the number of detail pages per search result page is reasonable. The worker task handles the entire flow from search to extracting all detail page HTMLs initiated by that search. The complexity shifts to ensuring the worker task manages this internal flow robustly. If a framework like Scrapy were used inside the worker, this would be its natural mode of operation. If using plain <code>requests</code>/<code>Playwright</code>, the worker code would need to loop or manage these sub-requests. (Decision should be finalized based on implementation complexity vs. desired granularity).</li> </ul>"},{"location":"scraping/strategy/","title":"Scraping Strategy Overview","text":"<p>This section details the overall strategy for identifying target websites, fetching data efficiently and ethically, handling common challenges, and storing the raw collected information. The goal is to create a robust pipeline for acquiring job listing data from diverse sources.</p> <p>Sub-sections cover:</p> <ul> <li>Website Identification &amp; robots.txt: How potential job boards and career pages are found and vetted.</li> <li>Targeting Strategy: Focusing scraping efforts using keywords and filters for efficiency.</li> <li>Fetching Implementation: Choosing the right tools (lightweight vs. browser automation) for data retrieval.</li> <li>Handling Scraping Challenges: Specific tactics for dealing with rate limits, dynamic content, pagination, and anti-scraping mechanisms.</li> <li>Raw Data Storage: The approach for storing unprocessed HTML content.</li> <li>Scrape Flow: Distinguishing between scraping search results and individual job details.</li> </ul>"},{"location":"scraping/targeting/","title":"Targeting Strategy (Keywords, Filters)","text":"<p>To maximize efficiency, reduce costs, and minimize load on target websites, scraping will be highly targeted rather than attempting full site crawls.</p>"},{"location":"scraping/targeting/#configuration-driven-targeting","title":"Configuration-Driven Targeting","text":"<p>The core strategy relies on the configuration stored in the central database (managed via Django Admin):</p> <ul> <li>Target Entities: Define specific combinations of:<ul> <li><code>Websites</code> (e.g., LinkedIn, Indeed, specific career page)</li> <li><code>Keywords</code> (e.g., \"Data Engineer\", \"Python\", \"React Developer\")</li> <li><code>Locations</code> (e.g., \"London\", \"New York\", \"Remote\")</li> </ul> </li> <li>Search Execution: The dispatcher service constructs specific search queries or navigates filter forms on the target website based on these configured combinations.</li> </ul>"},{"location":"scraping/targeting/#utilizing-website-filters","title":"Utilizing Website Filters","text":"<p>Whenever possible, the scraper will leverage built-in filtering mechanisms provided by the target websites.</p>"},{"location":"scraping/targeting/#date-filters","title":"Date Filters","text":"<ul> <li>Priority: This is a crucial filter for managing incremental scraping and freshness.</li> <li>Implementation: If a site allows filtering by \"posted within last 24 hours,\" \"posted last week,\" or similar, the scraper will be configured to use these options. The specific URL parameter or form interaction method for activating this filter will be stored in the website's configuration.</li> <li>Goal: Primarily collect new or recently updated job listings.</li> </ul>"},{"location":"scraping/targeting/#other-filters","title":"Other Filters","text":"<ul> <li>Keywords, location, job type (Full-time/Part-time), remote status filters provided by the site will be used directly during the search phase (Stage 1 scraping) as configured.</li> </ul>"},{"location":"scraping/targeting/#benefits-of-targeting","title":"Benefits of Targeting","text":"<ul> <li>Reduced Load: Significantly fewer pages are requested from target servers compared to broad crawling.</li> <li>Cost Savings: Lower usage of bandwidth, proxies, and compute resources (especially when using browser automation).</li> <li>Data Relevance: Focuses collection efforts on the most relevant job postings based on defined criteria.</li> <li>Scalability: Allows adding more specific targets without linearly increasing the load for existing targets.</li> </ul>"},{"location":"scraping/website_identification/","title":"Website Identification &amp; robots.txt","text":"<p>Identifying relevant sources is the first step in the data collection process.</p>"},{"location":"scraping/website_identification/#methods-for-identification","title":"Methods for Identification","text":"<p>A combination of methods will be used to discover potential sources for job listings:</p> <ul> <li>Search Engines: Utilizing targeted search queries (e.g., \"[Industry] job board\", \"jobs in [City] [Sector]\", \"careers at [Company Name]\").</li> <li>Industry Directories &amp; Lists: Consulting known lists of major job boards and industry-specific career sites.</li> <li>Competitor Analysis: Identifying where competitors or companies in target sectors post their jobs.</li> <li>Manual Curation: Adding specific company career pages or niche job boards based on project requirements.</li> </ul>"},{"location":"scraping/website_identification/#initial-vetting","title":"Initial Vetting","text":"<p>Before adding a website to the scraping configuration, a brief manual review is necessary to:</p> <ul> <li>Assess the relevance and volume of job listings.</li> <li>Understand the basic site structure (static vs. dynamic, pagination type).</li> <li>Check for clear terms of service regarding automated access.</li> <li>Crucially: Check the <code>robots.txt</code> file.</li> </ul>"},{"location":"scraping/website_identification/#respecting-robotstxt","title":"Respecting <code>robots.txt</code>","text":"<ul> <li>Process: Before any scraping attempt on a new domain, the system (or the initial setup process) must fetch and parse the <code>robots.txt</code> file located at the root of the domain (e.g., <code>https://www.example.com/robots.txt</code>).</li> <li>Adherence: The rules defined in <code>robots.txt</code> (specifically <code>User-agent</code> and <code>Disallow</code> directives) will be strictly adhered to. If <code>robots.txt</code> disallows scraping specific paths relevant to job listings for user agents matching our scraper's identity, those paths will not be scraped.</li> <li>Configuration: The configuration database might include a flag indicating if scraping is permitted based on <code>robots.txt</code>.</li> <li>Crawl-Delay: If a <code>Crawl-delay</code> directive is present, it will be respected as a minimum delay between requests, potentially overriding default politeness settings if it's stricter.</li> </ul> <p>Only websites deemed relevant, technically feasible to scrape (considering <code>robots.txt</code>), and aligned with project goals will be added to the configuration database for active scraping.</p>"}]}